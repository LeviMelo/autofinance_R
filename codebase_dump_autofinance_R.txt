Project structure for '/c/Users/Galaxy/LEVI/Projetos R/autofinance_R':
===============================================================================
  autofinance_backtest_core.R
  autofinance_config.R
  autofinance_db_core.R
  autofinance_fiscal_trading.R
  autofinance_ingest_b3.R
  autofinance_ingest_corporate.R
  autofinance_ingest_macro.R
  autofinance_ingest_splits.R
  autofinance_panel.R
  autofinance_portfolio_engine.R
  autofinance_risk_models.R
  autofinance_screener.R
  PLAN.md
  PLAN_risk.md



###############################################################################
### FILE: PLAN.md
###############################################################################
Here’s a concrete, implementable architecture you can drop into `PLAN.md` and build against.

---

# Autofinance V2 – Architectural Plan

## 0. Scope and Non-Negotiables

**Goal:**
Local, R-based “research station” to:

* Ingest and persist **B3 tape** (COTAHIST via `rb3`).
* Ingest and persist **corporate actions** (splits, and optionally dividends) via `quantmod::getSplits` / `getDividends`.
* Ingest and persist **macro factors** (CDI, Selic, IPCA, FX, etc.) from **BCB SGS**.
* Build **split-adjusted price panels** for B3 assets.
* Compute **research-grade metrics** (returns at multiple horizons, vol, skew, kurtosis, VaR, CVaR, drawdowns, betas to multiple factors).
* Run a **screener** (purely gross performance/risk, per-asset-class).
* Run **offline backtests** using only the local DB snapshot.
* Leave tax/fiscal rules and trading rules to the **portfolio/backtest layer**, not the screener.

**Non-negotiable constraints:**

1. **No always-on server, no cron jobs.** Everything is triggered by R functions you explicitly call.
2. **COTAHIST (via `rb3`) is the ground truth** for B3 OHLCV. Yahoo is never a primary price source, only a source of corporate actions (and optionally total-return stuff later).
3. **Splits are never inferred heuristically** from price jumps. Only `getSplits()` results are used. If we have no splits, we either:

   * Treat the series as unsplit, or
   * Flag it as “unverified” and optionally drop from serious analysis.
4. **`quantmod::getSplits()` is acceptable for the entire B3 universe**, with caching to SQLite. The real network constraint is bulk OHLC/dividend history from Yahoo, not splits.
5. **Backtests are 100% offline.** All required history must be in SQLite before the backtest starts.
6. **Screener works on gross world.** No tax logic, no fee modeling. Asset-class separation is allowed (Equity / FII / BDR / ETF buckets), but net-of-tax modeling happens later.

---

## 1. High-Level System Overview

Think of Autofinance V2 as four layers:

1. **Data Layer (SQLite + ingestion)**

   * `rb3` → COTAHIST → `prices_raw` table.
   * `quantmod` → splits/dividends → `corporate_actions` table.
   * BCB SGS → macro series → `macro_series` table.
   * Metadata for tickers (type, active/delisted, last update timestamps) → `assets_meta`.

2. **Adjustment + Panel Layer**

   * Merge **raw OHLCV** with **splits** to produce **split-adjusted OHLC** in memory for a given set of tickers and date range.

3. **Metrics & Screener Layer**

   * Given adjusted OHLC and macro series, compute a dense feature vector per asset: multi-horizon returns, volatilities, drawdown metrics, tail risk, betas to IBOV, SPX, USD, DI, etc.
   * Apply **liquidity filters** and **class-wise rankings** (e.g., Top K equities, Top K FIIs, Top K BDRs).

4. **Backtesting & Portfolio Layer**

   * Take screener outputs + DB snapshot.
   * Run offline simulations, with optional tax rules, costs, rebalancing rules, and constraints.
   * All data access via SQLite; no HTTP inside the simulation loop.

---

## 2. Core Principles and Design Invariants

1. **Single Source of Truth (per domain):**

   * Prices/volume: **B3** via `rb3`.
   * Corporate actions: **Yahoo** via `quantmod` (`getSplits`, maybe `getDividends`).
   * Macro factors: **BCB SGS**.

2. **SQLite as persistent state:**

   * All raw data and events live in a single file `autofinance.sqlite`.
   * No CSVs as canonical storage.

3. **On-demand synchronization:**

   * You call explicit sync functions:

     * `af_sync_b3()` for COTAHIST.
     * `af_sync_splits()` for splits.
     * `af_sync_macro()` for macro.
     * `af_sync_all()` as a convenience wrapper.
   * No background tasks.

4. **Adjustments computed locally:**

   * Use `TTR::adjRatios` (or equivalent) + splits table to adjust **all OHLC** (not only close).
   * Adjusted prices are computed on the fly when needed; they don’t need to be stored as a separate table initially (can be added later for speed).

5. **Dynamic horizons and lookback:**

   * Screener takes a **configurable lookback window**.
   * Only horizons compatible with available history are computed (e.g., no 3-year metrics if lookback is 1 year).

6. **Asset-class-aware ranking:**

   * The screener outputs **per-class rankings** (equities vs FIIs vs BDRs vs ETFs) instead of a single global “Top N”.

---

## 3. Data Sources and Contracts

### 3.1. B3 COTAHIST via `rb3`

* **Responsibility:** Provide daily OHLCV for all B3 tickers, including delisted ones.
* **Usage:**

  * Initial bootstrap: download all past years of interest.
  * On update: download only the missing year(s) and upsert rows.

### 3.2. Yahoo via `quantmod`

* **Splits (mandatory):**

  * `getSplits("PETR4.SA")`, etc.
  * Full-history for each ticker at first fetch, then incremental (from last date + 1).

* **Dividends (optional, later):**

  * `getDividends()`.
  * Only required if you want total-return metrics (DRIP, etc.) or realistic cash flow modeling.

### 3.3. BCB SGS Macro Series

Typical series:

* **Risk-free:** CDI, Selic.
* **Inflation:** IPCA, maybe IGP-M.
* **FX:** USD/BRL PTAX.
* **Activity:** IBC-Br.

Usage:

* For **betas/correlations** (asset returns vs macro factor returns).
* For regime-aware strategies in the backtester.

---

## 4. Data Model – SQLite Schema

This is the backbone. Explicit table definitions:

### 4.1. `assets_meta`

Tracks the universe and state per symbol.

**Columns:**

* `symbol` (TEXT, PK) — e.g., `"PETR4"`, `"KNCR11"`, `"IVVB11"`.
* `asset_type` (TEXT) — `"EQUITY"`, `"FII"`, `"BDR"`, `"ETF"`, etc.
* `sector` (TEXT, nullable).
* `active` (INTEGER) — `1` for currently trading, `0` if delisted.
* `last_update_prices` (TEXT, ISO date) — last date we ingested raw COTAHIST for this symbol (optional; COTAHIST is mostly year-based, so this can be global).
* `last_update_splits` (TEXT, ISO date) — last time we checked `getSplits`.
* `last_update_divs` (TEXT, ISO date) — last time we checked dividends (if used).

Indexes:

* PK (`symbol`) is enough for this table.

### 4.2. `prices_raw`

Immutable tape from B3.

**Columns:**

* `symbol` (TEXT).
* `refdate` (TEXT, `YYYY-MM-DD`).
* `open` (REAL).
* `high` (REAL).
* `low` (REAL).
* `close` (REAL).
* `vol_fin` (REAL) — financial volume.
* `qty` (INTEGER) — volume in shares/quotes.

**Constraints:**

* `PRIMARY KEY (symbol, refdate)`
  `WITHOUT ROWID` for efficiency.

Indexes:

* Composite PK already covers `(symbol, refdate)` queries.
* Optional index on `refdate` alone if you often query “all symbols on a given day”.

### 4.3. `corporate_actions`

Splits and (later) dividends.

**Columns:**

* `symbol` (TEXT).
* `date` (TEXT, `YYYY-MM-DD`) — ex-date.
* `type` (TEXT) — `"SPLIT"` or `"DIVIDEND"`.
* `value` (REAL) — split factor (e.g., `0.1` for 10→1) or dividend amount per share.

**Constraints:**

* `PRIMARY KEY (symbol, date, type)`.

Indexes:

* PK is enough.

### 4.4. `macro_series`

Macro indicators from BCB.

**Columns:**

* `series_id` (TEXT or INTEGER) — e.g. `"CDI"`, `"SELIC"`, `"IPCA"`, `"USD_P TAX"`, or numeric BCB ID.
* `refdate` (TEXT).
* `value` (REAL).

**Constraints:**

* `PRIMARY KEY (series_id, refdate)`.

### 4.5. Possible future tables (not mandatory now)

* `prices_adj_cache` — optional pre-materialized adjusted OHLC for speed.
* `metrics_cache` — optional per-symbol metric snapshots to avoid recomputing everything on each run (only if profiling shows it’s needed).

---

## 5. Codebase Layout (R Modules)

Organize the R code in modules, each with clear responsibilities.

### 5.1. `R/db_core.R`

**Purpose:** Low-level DB utilities.

Public functions:

* `af_db_connect(db_path = "data/autofinance.sqlite")`
  Opens connection, sets pragmas (`WAL`, cache size, etc.).
* `af_db_init(db_path)`
  Creates all tables if not present.

This module never knows about `rb3` or `quantmod`; it only does SQL.

### 5.2. `R/ingest_b3.R`

**Purpose:** Ingest COTAHIST via `rb3` into `prices_raw`.

Key functions:

* `af_sync_b3(years = NULL)`

  * If `years` is `NULL`, infer missing years from `prices_raw` max(refdate).
  * Uses `rb3::cotahist_get` to fetch full-year data.
  * Normalizes to `prices_raw` schema and upserts via `af_db_ingest_raw_b3()`.

* `af_db_ingest_raw_b3(dt_b3, con)`

  * Maps columns to schema and does `INSERT OR REPLACE` into `prices_raw`.

### 5.3. `R/ingest_splits.R`

**Purpose:** Fetch and store splits (and later dividends) via `quantmod`.

Key functions:

* `af_sync_splits(symbols = NULL, force = FALSE, max_age_days = 7)`

  * If `symbols` is `NULL`, select from `assets_meta` where `active = 1`.
  * For each symbol:

    * If `force = FALSE`, check `last_update_splits`. Skip if recent enough.
    * Call `quantmod::getSplits(paste0(symbol, ".SA"))`.
    * Convert to `data.table(symbol, date, value, type="SPLIT")`.
  * Upsert into `corporate_actions`.
  * Update `assets_meta.last_update_splits`.

### 5.4. `R/ingest_macro.R`

**Purpose:** Fetch macro series from BCB and store in `macro_series`.

Key functions:

* `af_sync_macro(series_ids, start_date, end_date)`

  * For each series_id, fetch missing data from BCB.
  * Upsert into `macro_series`.

### 5.5. `R/adjustment.R`

**Purpose:** Build adjusted OHLC panels from `prices_raw` + `corporate_actions`.

Key function:

* `af_build_adjusted_panel(symbols, start_date, end_date, con)`
  Steps:

  1. Query `prices_raw` for those symbols and date range.
  2. Query `corporate_actions` for splits only (`type = 'SPLIT'`).
  3. For each symbol:

     * Build an `xts` or `data.table` of OHLC.
     * Build a splits `xts`.
     * Compute adjustment ratios via `TTR::adjRatios` or equivalent.
     * Apply ratios to O/H/L/C.
  4. Return a single `data.table`:

     * `symbol, refdate, open_adj, high_adj, low_adj, close_adj, vol_fin, qty`.

No Yahoo price data here; only COTAHIST + splits.

### 5.6. `R/metrics.R`

**Purpose:** Given adjusted OHLC and macro series, compute metrics.

Main public function:

* `af_compute_metrics(panel_adj, macro_panel, config)`

Where:

* `panel_adj` is `data.table(symbol, refdate, open_adj, high_adj, low_adj, close_adj, vol_fin, qty)`.
* `macro_panel` is combined macro series and benchmark index returns.
* `config` defines:

  * `lookback_start`, `lookback_end`.
  * `horizons` (in days): e.g. `c(21, 63, 126, 252)`.
  * Which betas to compute (`ibov`, `usd`, `spx`, `di`).

Per symbol, it should compute (if history is long enough):

* **Returns:**

  * `ret_21d`, `ret_63d`, `ret_126d`, `ret_252d`, etc.
    (Skip any horizon > available days.)

* **Volatilities:**

  * `vol_21d`, `vol_252d` (annualized from daily sd).

* **Tail risk:**

  * `skew_252d`, `kurt_252d`.

* **VaR & CVaR (historical, 95%):**

  * Percentage loss threshold and conditional expectation below that.

* **Drawdowns:**

  * `max_dd_lookback`.
  * `ulcer_index` (sqrt of mean squared drawdowns).
  * `avg_time_underwater` (average length of consecutive drawdown periods).

* **Liquidity metrics:**

  * `median_vol_fin_63d` (or 252d).
  * `amihud_illiquidity` (average |return| / volume).

* **Betas/correlations:**

  * `beta_ibov`, `beta_usd`, `beta_spx`, `beta_di`:

    * Compute from daily return series via simple covariance/variance or regression.
  * Possibly `corr_ibov`, `corr_usd`, etc., for raw correlations.

Return shape: one row per symbol, one column per metric.

### 5.7. `R/screener.R`

**Purpose:** Glue: from DB → panel → metrics → ranked screener output.

Key function:

* `af_run_screener(config)`

Flow:

1. **Determine time window** from `config$lookback` (e.g. last 252 trading days).
2. **Query liquidity pre-filter** using `prices_raw` only (no adjustments yet):

   * Compute median `vol_fin` over lookback.
   * Keep only assets with `median_vol_fin >= config$min_liquidity`.
3. **Restrict universe** by `asset_type` (if desired).
4. **Build adjusted panel** for this filtered universe via `af_build_adjusted_panel()`.
5. **Load macro series** for that date range.
6. **Compute metrics** via `af_compute_metrics()`.
7. **Per asset type**, apply scoring/ranking:

   * Example: z-score of `ret_252d`, `vol_252d`, `ulcer_index`, `beta_ibov`, etc., with your weighting scheme.
8. Return a **list** or long data.frame:

   * `list(equity = df_equity_ranked, fii = df_fii_ranked, bdr = df_bdr_ranked, etf = df_etf_ranked)`.

Screener does **not** apply tax or trading rules.

### 5.8. `R/backtest.R`

**Purpose:** Run offline simulations based on DB.

Key pieces:

* `af_backtest_prepare(symbols, start_date, end_date, con)`

  * Uses `af_build_adjusted_panel()` to produce full adjusted history for all needed symbols and time range.
  * Optionally adds dividend cash flows if you support them.

* `af_backtest_run(strategy_fn, universe, start_date, end_date, config)`

  * `strategy_fn` is a user-supplied function that:

    * Given date `t` and current state, returns target weights or trades.
  * The engine:

    * Steps through time (e.g. daily or rebalance frequency).
    * Updates portfolio, applies costs, optionally taxes.
    * Produces equity curve and statistics.

Inside `af_backtest_run`, **no HTTP calls**. All data comes from the `panel_adj` prepared beforehand.

### 5.9. `R/portfolio.R` (later)

This is where you implement:

* Tax rules (BDR vs equity vs FIIs).
* Capital gains vs dividends modeling.
* Specific allocation schemes (risk parity, max Sharpe, constraints, position limits).

Screener outputs feed into portfolio construction, but screener itself remains tax-agnostic.

### 5.10. `R/config.R`

Central place for defaults:

* DB path.
* Default lookback window.
* Horizons.
* Macro series mapping.
* Liquidity thresholds.

---

## 6. Data Flows (Step-by-Step)

### 6.1. Initial Bootstrap

1. `af_db_init()`
2. `af_sync_b3(years = 2010:current_year)`
   → fills `prices_raw`.
3. Populate `assets_meta`:

   * From B3 info or manually (asset_type, active, etc.).
4. `af_sync_splits()`

   * Full universe splits → `corporate_actions`.
5. `af_sync_macro()`

   * Fill `macro_series` with CDI, Selic, IPCA, USD, etc.

After this, you have a complete local snapshot.

### 6.2. On-Demand Update Before Analysis

Any time you want to rebalance / re-run:

1. `af_sync_b3()` (only missing years or days; COTAHIST makes this cheap).
2. `af_sync_macro()` (only missing dates).
3. `af_sync_splits()` (only stale symbols by `last_update_splits` threshold).

Then:

* `af_run_screener(config)` for fresh rankings.
* Or `af_backtest_run(...)` on the current DB snapshot.

### 6.3. Screener Execution Flow (Detailed)

Given `config`:

1. Determine date range `[t0, t1]` based on lookback.
2. From `prices_raw`:

   * For each symbol in `assets_meta[active=1]`, compute `median_vol_fin` over `[t0, t1]`.
   * Filter: `median_vol_fin >= min_liquidity`.
3. Build adjusted OHLC for filtered symbols via `af_build_adjusted_panel()`.
4. Load macro series for `[t0, t1]`.
5. Compute metrics via `af_compute_metrics()`:

   * Skip horizons that exceed available history for each symbol.
6. Split metrics by `asset_type` and rank within each class.

---

## 7. Core Algorithms (Implementation Notes)

### 7.1. Split Adjustment

Given:

* Raw OHLC series `P_t` (open, high, low, close).
* Splits: dates `s_i` with ratios `r_i` (as given by `getSplits`, e.g. `0.1` for 10→1).

Use `TTR::adjRatios(P, splits)` to compute a cumulative adjustment factor `A_t` such that:

* For each date `t`, `P_adj,t = P_t * A_t`.

Apply `A_t` identically to `open`, `high`, `low`, `close` so the OHLC range remains coherent.

### 7.2. Returns and Volatility

Daily log return:

* `r_t = log(close_adj_t / close_adj_{t-1})`.

Horizon return over N days (geometric):

* `R_N = exp(sum_{i=1..N} r_{t-i+1}) - 1`.

Annualized vol over window N:

* `vol_N = sd(r_{window}) * sqrt(252)`.

### 7.3. VaR and CVaR (Historical, 95%)

Given a vector of daily returns `r` in the window:

* Sort `r` ascending.
* `VaR_95` = 5th percentile (e.g., `quantile(r, probs = 0.05)`).
* `CVaR_95` = mean of returns below `VaR_95`.

### 7.4. Drawdowns and Ulcer Index

Equity curve from price (normalized):

* `E_t = close_adj_t / close_adj_{t0}`.

Running max:

* `M_t = max_{k <= t} E_k`.

Drawdown:

* `DD_t = (E_t - M_t) / M_t` (negative or zero).

Max drawdown:

* `min(DD_t)`.

Ulcer index:

* `UI = sqrt(mean(DD_t^2))` over the window.

Time underwater:

* Identify contiguous runs where `DD_t < 0`.
* Take average length of those runs.

### 7.5. Betas to Factors

Given asset returns `r_a,t` and factor returns `r_f,t` for aligned dates:

Beta:

* `beta = cov(r_a, r_f) / var(r_f)`.

You can also run a regression:

* `r_a = alpha + beta * r_f + error`, but for speed, covariance/variance is enough.

Compute betas against:

* IBOV (or IBOV11/IBOV index series).
* SPX (converted to BRL if needed).
* USD/BRL FX.
* DI or proxy fixed-income index.

---

## 8. Configuration Layer

Either:

* A simple `list` in R (`af_default_config()`), or
* A YAML file that gets parsed once per session.

Key config fields:

* `db_path`.
* `lookback_days` (e.g., 252).
* `horizons_days` (e.g., `c(21, 63, 126, 252)`).
* `min_liquidity` (e.g., 500000 BRL median daily volume).
* `macro_series_ids`.
* `benchmarks` (e.g., `"IBOV"`, `"SPX"`, `"USD"`, `"CDI"`).

---

## 9. Validation & QA Strategy

You need systematic sanity checks.

### 9.1. Price sanity

* Randomly sample symbols.
* Compare your adjusted close vs Yahoo Adjusted Close for those symbols and dates (one-off diagnostic script).
* Flag if deviations exceed some tolerance (e.g., >1–2%).

### 9.2. Corporate actions sanity

* For a few known split events (ABEV3, GOGL34, etc.), verify:

  * Splits appear in `corporate_actions`.
  * Adjusted price curve behaves as expected across split dates (no fake crash).

### 9.3. Macro sanity

* Plot each macro series; visually check for obviously broken segments (zeros, missing months).

### 9.4. Metric sanity

For a single symbol:

* Manually compute a subset of metrics (e.g., 21d return, 252d vol, max drawdown) in a separate R script and compare to `af_compute_metrics` output.

---

## 10. Roadmap / Phases

### Phase 1 – DB + B3 Ingest

* Implement `db_core.R`, `ingest_b3.R`.
* Get `prices_raw` populated from 2010 to present.

### Phase 2 – Splits Ingest + Adjustment

* Implement `ingest_splits.R` and `adjustment.R`.
* Verify adjusted prices for a small set of symbols vs Yahoo.

### Phase 3 – Macro Ingest

* Implement `ingest_macro.R`.
* Store CDI, Selic, IPCA, USD, etc., in `macro_series`.

### Phase 4 – Metrics Engine

* Implement `metrics.R`.
* Test per symbol metrics thoroughly.

### Phase 5 – Screener

* Implement `screener.R`.
* End-to-end flow: DB → screener → per-class rankings.

### Phase 6 – Backtest Core

* Implement `backtest.R` with a simple long-only strategy to validate the engine.
* Confirm it runs offline and uses only SQLite + in-memory panels.

### Phase 7 – Portfolio & Fiscal Logic (if/when you care)

* Implement tax rules, position limits, rebalancing logic in `portfolio.R`.
* Integrate backtester with fiscal modeling.

---

This is the architecture. If you follow this as `PLAN.md`, you’ll have a clean separation between:

* **Data acquisition & storage** (B3, Yahoo splits, BCB),
* **Price adjustment & panel construction**,
* **Metric computation & screening**,
* **Offline simulation & portfolio logic**.

From here, next concrete step is Phase 1: write `af_db_init()` and `af_sync_b3()` exactly against the schema above, and we can start hardening from there.



###############################################################################
### FILE: PLAN_risk.md
###############################################################################
Perfeito, vamos atualizar o plano levando **GARCH/DCC/VAR + portfolio engine + backtest** para o centro da arquitetura, não como “futuro opcional”.

Vou te entregar um `PLAN.md` já revisado, autocontido e consistente com:

* DB + COTAHIST como base.
* Screener “research-grade”.
* **Risk models avançados (GARCH/DCC/VAR) como primeira classe**.
* Portfolio engine com restrições.
* Backtest core que cola tudo isso **ao longo do tempo**.
* Camada fiscal/trading construída em cima do backtest.

---

# Autofinance – Full Architecture (DB + Screener + VAR/GARCH/DCC + Portfolio + Backtest)

## 0. Escopo e objetivos

**Objetivo global:**
Construir um sistema local, em R, que funcione como uma **estação quantitativa de pesquisa e simulação** para ativos da B3 (ações, FIIs, BDRs, ETFs), com:

1. **Camada de dados sólida**:

   * COTAHIST via `rb3` como **fita oficial** de OHLCV.
   * Ações corporativas (splits, depois dividendos) via `quantmod::getSplits` / `getDividends`.
   * Fatores macro via **BCB SGS** (CDI, Selic, IPCA, FX, etc.).
   * Tudo persistido em **SQLite** (sem CSVs frágeis).

2. **Screener avançado**:

   * Trabalha sobre preços ajustados por splits.
   * Usa métricas densas: retornos em múltiplos horizontes, volatilidade, skew/kurtosis, VaR/CVaR, drawdown, Ulcer Index, betas a IBOV, SPX, USD, DI, etc.
   * Faz ranking **por classe de ativo** (Equity/FII/BDR/ETF), sem misturar tudo num top global tosco.

3. **Modelos de risco/retorno avançados (centrais, não laterais)**:

   * **GARCH** univariado para volatilidades condicionais σᵢ,t.
   * **DCC** para correlações condicionais Cᵗ e covariância Σₜ = Dₜ Cₜ Dₜ.
   * **VAR** para dinâmica conjunta de retornos ou fatores e previsão de μₜ (expected returns).
   * Tudo empacotado num módulo `autofinance_risk_models.R` com interface clara para o portfolio engine.

4. **Portfolio engine sério**:

   * Respeita restrições: long-only, w_max por ativo, limite de alavancagem, possíveis restrições por classe (mínimo FIIs, máximo BDRs, etc.).
   * Suporta múltiplos modos: equal, inv_vol, min_var, mean_var, max_sharpe.
   * Usa μ e Σ vindos dos risk models (sample/shrinkage/GARCH+DCC/VAR).

5. **Backtest core**:

   * Loop temporal com janelas de lookback (ex: 3 anos) para recalibrar screener + risk models + portfolio em cada rebalance.
   * Usa **somente SQLite + memória**; sem chamadas HTTP dentro da simulação.
   * Entrega curva de patrimônio, trades, stats, decomposição por período.

6. **Camada fiscal/trading**:

   * Aplica regras de impostos (ações x BDR x FIIs), custos de transação, limites operacionais **sobre o backtest**.
   * Não contamina screener nem risk models com fiscal.

---

## 1. Camadas e módulos

### 1.1 Visão em camadas

1. **Data & DB layer**

   * Arquivos:

     * `autofinance_db_core.R`
     * `autofinance_ingest_b3.R`
     * `autofinance_ingest_splits.R`
     * `autofinance_ingest_macro.R`
   * Responsável por:

     * Sincronizar COTAHIST → `prices_raw`.
     * Sincronizar splits/dividends → `corporate_actions`.
     * Sincronizar macro → `macro_series`.
     * Manter `assets_meta`.

2. **Adjustment & Panel layer**

   * Arquivo: `autofinance_panel.R`
   * Responsável por:

     * Construir painéis de preços **ajustados por split** (OHLC) a partir de `prices_raw` + `corporate_actions`.
     * Calcular retornos simples/log, excess returns vs RF, agregar macros.

3. **Screener layer**

   * Arquivo: `autofinance_screener.R` (ou `autofinance_universe_screener.R`)
   * Responsável por:

     * Filtrar universo por liquidez/ativos válidos.
     * Calcular todo o vetor de métricas cross-section.
     * Rankear por classe de ativo.

4. **Risk models layer (μ, Σ)**

   * Arquivo: `autofinance_risk_models.R`
   * Responsável por:

     * Estimar **Σ (covariância)** por vários métodos: sample, shrinkage, GARCH+DCC.
     * Estimar **μ (expected returns)**: média histórica, momentum-based, VAR-based.
     * Fornecer interfaces estáveis pro portfolio engine.

5. **Portfolio engine layer**

   * Arquivo: `autofinance_portfolio_engine.R`
   * Responsável por:

     * Dado μ, Σ + restrições, resolver problemas de alocação (equal, inv_vol, min_var, mean_var, max_sharpe).
     * Garantir long-only, w_max, leverage_max, restrições por grupo.

6. **Backtest core layer**

   * Arquivo: `autofinance_backtest_core.R`
   * Responsável por:

     * Loop temporal: em cada rebalance, rodar screener, risk model, portfolio engine usando só dados até t.
     * Aplicar retornos realizados entre t e t_next.
     * Calcular métricas de performance, drawdown, turnover, etc.

7. **Fiscal & trading rules layer**

   * Arquivo: `autofinance_fiscal_trading.R`
   * Responsável por:

     * Modelar impostos e custos em cima de sequências de trades/posições.
     * Dar versão “net-of-tax” da curva de patrimônio produzida pelo backtest core.

---

## 2. Data & DB – Modelo e ingestão

### 2.1 Esquema SQLite (resumo consolidado)

**Tabela `assets_meta`**

* `symbol` (PK) – `"PETR4"`, `"IVVB11"`, `"KNCR11"`, `"A1IV34"`, etc.
* `asset_type` – `"EQUITY" | "FII" | "BDR" | "ETF" | ...`.
* `sector` – texto opcional (B3 sector/segmento).
* `active` – `1` ativo, `0` deslistado (mantém para evitar survivorship bias).
* `last_update_splits` – data da última checagem do `getSplits`.
* `last_update_divs` – idem para dividendos (quando usado).

**Tabela `prices_raw`** – COTAHIST

* `symbol`
* `refdate` (YYYY-MM-DD)
* `open, high, low, close` (REAL)
* `vol_fin` (REAL)
* `qty` (INTEGER)
* **PK** `(symbol, refdate)` `WITHOUT ROWID`.

**Tabela `corporate_actions`**

* `symbol`
* `date` (ex-date)
* `type` – `"SPLIT"` / `"DIVIDEND"`.
* `value` – razão do split (ex: 0.1) ou valor do dividendo.
* **PK** `(symbol, date, type)`.

**Tabela `macro_series`**

* `series_id` – ex: `"CDI"`, `"SELIC"`, `"IPCA"`, `"USD_P TAX"`, ou ID numérico BCB.
* `refdate`
* `value`
* **PK** `(series_id, refdate)`.

### 2.2 Ingestão e sincronização (on-demand, sem servidor)

**Funções principais:**

* `af_db_init()`
  Cria o banco e as tabelas se não existirem.

* `af_sync_b3(years = NULL)`

  * Se `years` é `NULL`, checa `max(refdate)` em `prices_raw` e baixa apenas anos que têm datas faltantes (ex: ano corrente).
  * Usa `rb3::cotahist_get` para pegar COTAHIST e insere via `INSERT OR REPLACE`.

* `af_sync_splits(symbols = NULL, force = FALSE, max_age_days = 7)`

  * Se `symbols = NULL`, pega da `assets_meta` (ativos ativos).
  * Para cada symbol, checa `last_update_splits`:

    * Se recente e `force = FALSE` → pula.
    * Caso contrário, chama `quantmod::getSplits(paste0(symbol, ".SA"))` (full history ou incremental, a gosto).
  * Normaliza em `corporate_actions` com `type="SPLIT"`, atualiza `last_update_splits`.

* `af_sync_dividends(...)` (opcional, depois)
  Mesma lógica, mas para dividendos.

* `af_sync_macro(series_ids, start_date, end_date)`

  * Para cada série do BCB, busca dados faltantes e injeta em `macro_series`.

---

## 3. Panel e retornos

### 3.1 Construção do painel ajustado

Arquivo: `autofinance_panel.R`

Função central:

```r
af_build_adjusted_panel <- function(symbols, start_date, end_date, con) {
  # 1) Query prices_raw
  # 2) Query corporate_actions (SPLIT)
  # 3) Para cada symbol:
  #    - OHLC xts raw
  #    - splits xts
  #    - adjRatios -> fatores de ajuste
  #    - aplica em O/H/L/C
  # 4) Devolve data.table com:
  #    symbol, refdate, open_adj, high_adj, low_adj, close_adj, vol_fin, qty
}
```

### 3.2 Retornos e excess returns

Ainda em `autofinance_panel.R` (ou submódulo):

* `af_compute_returns(panel_adj, rf_series = NULL)`:

  * Calcula:

    * `r_simple_t = close_adj_t / close_adj_{t-1} - 1`.
    * `r_log_t = log(close_adj_t / close_adj_{t-1})`.
  * Se `rf_series` (CDI, Selic) fornecido:

    * Calcula `excess_ret = r_simple - r_rf_diário`.

* Pode opcionalmente já acrescentar:

  * Benchmarks (ex: IBOV, SPX_BR, USD) na mesma estrutura.

---

## 4. Screener – métrica, filtros, ranking

Arquivo: `autofinance_screener.R` / `autofinance_universe_screener.R`

### 4.1 Fluxo conceitual

1. **Definir janela de lookback**: ex: últimos 252 dias úteis.
2. **Filtrar universo por liquidez** usando `prices_raw`:

   * `median_vol_fin_lookback >= min_liquidity`.
   * Opcional: days_traded_ratio, Amihud, etc.
3. **Construir painel ajustado** para esse universo e janela via `af_build_adjusted_panel`.
4. **Carregar macro & benchmarks** para o período.
5. **Calcular vetor de métricas** por símbolo.
6. **Rankear por classe de ativo** conforme config de pesos/fatores.

### 4.2 Vetor de métricas (cross-section)

Por símbolo (dado painel ajustado e retornos):

* **Retornos / momentum**:

  * Janela de lookback `T` (dias).
  * Conjunto de horizontes `H = {21, 63, 126, 252, ...}`.
  * Para cada `h ∈ H` tal que `h <= T`:

    * `ret_h` = retorno acumulado geométrico nos últimos h dias.
    * Opcional: “ret_rel_ibov_h” = retorno do ativo – retorno do IBOV no mesmo período.

* **Volatilidades**:

  * `vol_21d`, `vol_63d`, `vol_252d` (annualizadas).

* **Distribuição / cauda**:

  * `skew_252d`.
  * `kurt_252d`.

* **Risco de cauda**:

  * `var_95_252d`.
  * `cvar_95_252d`.

* **Drawdowns**:

  * `max_dd_lookback`.
  * `ulcer_index` (UI).
  * `avg_time_underwater`.

* **Liquidez**:

  * `median_vol_fin_63d` (ou 252d).
  * `amihud_illiquidity` = média(|ret| / vol_fin).
  * `days_traded_ratio` = dias com `qty > 0` / total de dias.

* **Betas e correlações**:

  * Contra IBOV, USD, SPX, DI, etc., usando retorno diário:

    * `beta_ibov`, `beta_usd`, `beta_spx`, `beta_di`.
    * `corr_ibov`, `corr_usd`, etc.

### 4.3 Interface do screener

```r
af_screener_config_default <- list(
  lookback_days  = 252L,
  horizons_days  = c(21L, 63L, 126L, 252L),
  min_liquidity  = 5e5,    # BRL
  min_days_traded = 0.8,   # 80% dos dias com negócio
  score_weights  = list(
    ret_252d      = +1.0,
    vol_252d      = -0.5,
    ulcer_index   = -0.7,
    beta_ibov     = -0.2,
    amihud        = -0.3
    # etc, ajustáveis
  )
)

af_run_screener <- function(config = af_screener_config_default, con) {
  # 1) determina janela
  # 2) filtra liquidez usando prices_raw
  # 3) panel_adj + returns + macro
  # 4) metrics
  # 5) ranking por asset_type
  # 6) retorna lista:
  #    list(equity = df_e, fii = df_f, bdr = df_b, etf = df_etf)
}
```

---

## 5. Risk Models – μ e Σ (incluindo GARCH/DCC/VAR)

Arquivo: `autofinance_risk_models.R`

Meta: separar claramente **cálculo de métricas cross-section** (screener) de **modelagem de μ e Σ** para otimização de portfólio.

### 5.1 Interface geral

```r
af_risk_config_default <- list(
  cov_method   = "sample",      # "sample", "shrinkage", "garch_dcc"
  mu_method    = "mean",        # "mean", "momentum", "var"
  window_years = 3,
  rf_series_id = "CDI"
)

af_risk_estimate <- function(
  panel_returns,  # retornos diários (já ajustados), subset de símbolos
  macro_panel,    # opcional, para VAR/fatores
  config = af_risk_config_default
) {
  # 1) Seleciona janela (ex: últimos 3 anos)
  # 2) Estima Σ conforme cov_method
  # 3) Estima μ conforme mu_method
  # 4) Retorna list(mu = μ_vec, Sigma = Σ_mat, meta = list(...))
}
```

### 5.2 Covariance (Σ)

**cov_method = "sample"**
Covariância de amostra simples de excess returns:

* Σ = cov(R_excesso).

**cov_method = "shrinkage"**
Ex: Ledoit-Wolf:

* Σ_shrink = λ·F + (1 − λ)·Σ_sample,
  onde F pode ser matriz diagonal (variâncias apenas) ou “identidade escalada”.

**cov_method = "garch_dcc"** (núcleo avançado):

1. Para cada ativo i:

   * Ajusta um GARCH(1,1) a rᵢ,t:

     * σᵢ,t² = ω + α εᵢ,t−1² + β σᵢ,t−1²
   * Obtém resíduos padronizados:

     * zᵢ,t = εᵢ,t / σᵢ,t.

2. Conjunto de z_t = (z₁,t, z₂,t, ..., zₙ,t):

   * Ajusta modelo DCC para a matriz de correlações condicionais Cₜ.

3. Em um rebalance em t*:

   * Pegamos σᵢ,t* para cada ativo → Dₜ* = diag(σ₁,t*,...,σₙ,t*).
   * Pegamos Cₜ* do DCC.
   * Covariância condicional:

     * Σₜ* = Dₜ* Cₜ* Dₜ*.

**Design importante:**
No backtest, **não recalibrar GARCH/DCC todo dia**; recalibrar **por rebalance** (ex.: mensal). Para cada rebalance t:

* Usar janela (ex.: últimos 3 anos) para calibrar GARCH/DCC.
* Extrair Σₜ pronto para o portfolio engine.

### 5.3 Expected returns (μ)

**mu_method = "mean"**
Média histórica de excess returns na janela:

* μᵢ = média(rᵢ_excesso) (anualizada) para cada ativo.

**mu_method = "momentum"**
Imputar μ com base em métricas de momentum vindas do screener:

* μᵢ ∝ combinação linear de `ret_21d`, `ret_63d`, `ret_252d`.
* Pode normalizar e calibrar escalas.

**mu_method = "var"** (VAR-based, avançado):

1. Definir vetor de variáveis para o VAR:

   * Pode ser:

     * diretamente retornos de ativos, ou
     * retornos de fatores (IBOV, SPX_BR, FX, yield, etc.) e mapear ativos para fatores.

2. Ajustar VAR(p):

   * yₜ = A₁ yₜ₋₁ + ... + Aₚ yₜ₋ₚ + εₜ.

3. Prever **E[yₜ₊₁ | info até t]**:

   * μ_fatores,t+1 para fatores.
   * Se usar fatores, projetar μ_fatores em μ_ativos usando exposições de fator (betas, regressão cross-section).

4. Resultado:

   * μ_t (vector) com expected returns condicionais.

No contexto do backtest, também recalibrado **por rebalance**, usando janela móvel.

---

## 6. Portfolio Engine – alocação com restrições

Arquivo: `autofinance_portfolio_engine.R`

### 6.1 Config

```r
af_port_default_config <- list(
  cov_method    = "sample",      # repassado para risk_models
  mu_method     = "mean",        # idem
  window_years  = 3,
  mode          = "min_var",     # "equal", "inv_vol", "min_var", "mean_var", "max_sharpe"
  long_only     = TRUE,
  w_max         = 0.20,          # máximo por ativo
  leverage_max  = 1.0,           # sum(w_i) <= 1, sem alavancagem
  group_constraints = NULL       # ex: list(FII_min = 0.2, BDR_max = 0.3)
)
```

### 6.2 Interface

```r
af_build_portfolio <- function(
  panel_returns,     # retornos diários para selected_symbols
  selected_symbols,  # vetores de tickers escolhidos (p.ex. top N do screener)
  port_config,
  macro_panel = NULL # se risk_models precisar
) {
  # 1) Chama af_risk_estimate() com methods do port_config
  # 2) Resolve problema de otimização conforme mode:
  #    - equal, inv_vol, min_var, mean_var, max_sharpe
  # 3) Aplica restrições: long_only, w_max, leverage_max, group_constraints
  # 4) Retorna list(weights, cov_mat, mu_vec, stats)
}
```

### 6.3 Modos de alocação

* **"equal"**:

  * wᵢ = 1/N com restrições de grupo (se houver).

* **"inv_vol"**:

  * wᵢ ∝ 1/σᵢ (σᵢ pode vir de Σ ou de estimador simples).

* **"min_var"**:

  * Resolver:

    * min wᵀ Σ w
      s.a.

      * wᵢ ≥ 0 (se long_only)
      * ∑ wᵢ = 1
      * wᵢ ≤ w_max
      * * group_constraints (por ex. ∑ wᵢ (i em BDR) ≤ 0.3).

* **"mean_var"**:

  * Problema de Markowitz:

    * max [wᵀ μ − λ · wᵀ Σ w], para algum λ (aversion risk).

* **"max_sharpe"**:

  * max ( (wᵀ μ − r_f) / √(wᵀ Σ w) )
    com mesmas restrições.

Implementação com `quadprog`, `ROI` + plugins, ou `nloptr`, conforme preferência.

---

## 7. Backtest Core – colando screener + risk + portfolio ao longo do tempo

Arquivo: `autofinance_backtest_core.R`

### 7.1 Config geral

```r
af_backtest_config_default <- list(
  rebalance_freq = "monthly",    # "monthly", "quarterly", etc.
  lookback_years = 3,
  screener_config = af_screener_config_default,
  risk_config     = af_risk_config_default,
  port_config     = af_port_default_config,
  rf_series_id    = "CDI"
)
```

### 7.2 Interface

```r
af_backtest <- function(
  symbols_universe,    # pode ser NULL -> usar assets_meta ativos
  start_date,
  end_date,
  bt_config = af_backtest_config_default,
  con
) {
  # 1) Prepara painel ajustado + retornos + macros (de af_build_adjusted_panel / af_compute_returns)
  # 2) Gera sequência de datas de rebalance (ex: fim de cada mês)
  # 3) Loop ao longo dos rebalances:
  #    - t_reb é data de rebalance
  #    - define janela [t_reb - lookback_years, t_reb] (sem look-ahead)
  #    - roda screener nessa janela
  #    - escolhe selected_symbols
  #    - chama af_build_portfolio(...) para obter w_t
  #    - aplica retornos realizados de (t_reb, t_next] para atualizar carteira
  #    - registra trades, equity, etc.
  # 4) Retorna list(equity_curve, trades, stats, by_period)
}
```

### 7.3 Propriedades importantes

* **Sem HTTP** dentro do loop:

  * Toda a sincronização (`af_sync_*`) deve ter ocorrido **antes** do backtest.
  * O backtest só lê do SQLite e trabalha em memória.

* **Sem look-ahead**:

  * Screener, risk_models, portfolio engine sempre usam dados apenas até t_reb.

* **Recalibração por rebalance**:

  * GARCH/DCC/VAR são recalibrados apenas em t_reb, usando janela de lookback (ex: 3 anos).
  * Isso torna o custo computacional administrável.

---

## 8. Fiscal & Trading Rules – camada “net-of-tax”

Arquivo: `autofinance_fiscal_trading.R`

### 8.1 Papel

Não altera screener, Σ ou μ.
Pega a **sequência de trades/posições** do backtest e aplica:

* Regras de IR para:

  * Ações locais (isenção dos 20k, 15% ganho, etc., conforme legislação que você quiser codar).
  * BDRs (sem isenção, alíquotas diferentes).
  * FIIs (tratamento de rendimentos vs ganho de capital).
* Custos operacionais:

  * Corretagem, emolumentos, spread, etc.

### 8.2 Interface

```r
af_apply_fiscal_trading <- function(
  trades,          # data.table: date, symbol, qty, price, side
  rf_series = NULL,
  rules_config
) {
  # 1) Reconstruir posição por ativo ao longo do tempo
  # 2) Identificar eventos tributáveis por classe
  # 3) Calcular imposto devido, custos, eventualmente acumular "caixa"
  # 4) Ajustar equity_curve do backtest para "net-of-tax"
  # 5) Retornar curva líquida + stats
}
```

---

## 9. Configuração central e orquestração

Arquivo: `autofinance_config.R`

* `af_default_paths` – caminho do DB.
* `af_screener_config_default`.
* `af_risk_config_default`.
* `af_port_default_config`.
* `af_backtest_config_default`.

Função helper:

```r
af_sync_all <- function(con) {
  af_sync_b3(con = con)
  af_sync_macro(con = con, ...)
  af_sync_splits(con = con)
  # (dividends mais tarde, se/quando usar)
}
```

---

## 10. Roadmap de desenvolvimento com GARCH/DCC/VAR já planejados

### Fase 1 – DB + B3

* Implementar `af_db_init`, `af_db_connect`.
* Implementar `af_sync_b3` + ingestão COTAHIST → `prices_raw`.
* Preencher `assets_meta` inicial (tipo de ativo, active/deslistado).

### Fase 2 – Splits + painel ajustado

* Implementar `af_sync_splits` → `corporate_actions` (SPLIT).
* Implementar `af_build_adjusted_panel` e validar com alguns tickers vs Yahoo Adjusted Close.

### Fase 3 – Macro

* Implementar `af_sync_macro` (CDI, Selic, IPCA, USD, IBOV, etc.).
* Validar séries.

### Fase 4 – Retornos + Screener base

* Implementar `af_compute_returns`.
* Implementar `af_compute_metrics` (momentum, vol, drawdown, betas simples, etc.).
* Implementar `af_run_screener` com ranking por classe.

### Fase 5 – Risk models (sample & shrinkage) + portfolio engine básico

* Implementar `af_risk_estimate` com `cov_method = "sample"/"shrinkage"`, `mu_method = "mean"`.
* Implementar `af_build_portfolio` com modos "equal", "inv_vol", "min_var" sob restrições básicas.
* Validar com testes unitários e casos simples.

### Fase 6 – GARCH + DCC

* Integrar `rugarch` para GARCH univariado.
* Integrar `rmgarch` para DCC.
* Implementar `cov_method = "garch_dcc"`:

  * Por rebalance: calibrar GARCH/DCC na janela de lookback.
  * Extrair Σₜ condicional.

### Fase 7 – VAR para μ

* Definir vetor de variáveis (retornos de fatores ou ativos principais).
* Usar `vars` ou equivalente para ajustar VAR(p).
* Implementar `mu_method = "var"`:

  * Prever μₜ+₁ para fatores.
  * Mapear μ_fatores → μ_ativos (via betas/factor loadings).

### Fase 8 – Backtest core

* Implementar `af_backtest`:

  * Loop por rebalance.
  * Em cada rebalance:

    * screener → selected_symbols
    * risk_models → μ, Σ
    * portfolio_engine → weights
  * Aplicar retornos, registrar trades, equity, stats.

### Fase 9 – Fiscal/trading

* Implementar `af_apply_fiscal_trading`.
* Configurar regras de impostos/custos para Ações, FIIs, BDRs.
* Gerar curva bruta vs curva líquida.

### Fase 10 – Tuning e grid search

* Sobre o backtest core:

  * Rodar variações de `score_weights` do screener, métodos de Σ e μ.
  * Encontrar combinações que maximizem métricas (Sharpe, MAR, etc.).
  * Isso vira um hyperparameter search, não “ML”, mas suficiente para calibrar o sistema.

---

Esse plano já assume, desde o começo, que:

* **GARCH/DCC e VAR são cidadãos de primeira classe** no módulo de modelos de risco.
* O **portfolio engine** é construído explicitamente para receber Σₜ e μₜ condicional desses modelos.
* O **backtest** é o orquestrador real, com janelas móveis e re-estimação em cada rebalance.
* A camada fiscal/trading é acoplada apenas ao nível de trades/posições, sem contaminar screener ou risk models.

Se você quiser, o próximo passo natural é pegarmos **Fase 1–2** e já rascunharmos os skeletons de funções e arquivos (sem implementação pesada) para começar a preencher o projeto de forma consistente com esse `PLAN.md`.



###############################################################################
### FILE: autofinance_backtest_core.R
###############################################################################
############################################################
# autofinance_backtest_core.R
# Time-series backtest: screener + risk + portfolio
############################################################

# Compute rebalance dates (e.g. last trading day of each month)
af_bt_compute_rebalance_dates <- function(panel,
                                          start_date,
                                          end_date,
                                          rebalance_freq = "monthly",
                                          lookback_years = 3L) {
  stopifnot("refdate" %in% names(panel))
  dt <- data.table::copy(panel)
  dt[, refdate := as.Date(refdate)]

  dates <- sort(unique(dt$refdate))
  dates <- dates[dates >= start_date & dates <= end_date]
  if (!length(dates)) stop("No trading dates in panel for given backtest range.")

  dt <- data.table::data.table(refdate = dates)

  if (rebalance_freq == "monthly") {
    dt[, `:=`(
      year  = lubridate::year(refdate),
      month = lubridate::month(refdate)
    )]
    reb <- dt[, .(reb_date = max(refdate)), by = .(year, month)][order(reb_date), reb_date]
  } else if (rebalance_freq == "quarterly") {
    dt[, `:=`(
      year    = lubridate::year(refdate),
      quarter = lubridate::quarter(refdate)
    )]
    reb <- dt[, .(reb_date = max(refdate)), by = .(year, quarter)][order(reb_date), reb_date]
  } else if (rebalance_freq == "weekly") {
    dt[, `:=`(
      year = lubridate::year(refdate),
      week = lubridate::isoweek(refdate)
    )]
    reb <- dt[, .(reb_date = max(refdate)), by = .(year, week)][order(reb_date), reb_date]
  } else {
    stop("Unsupported rebalance_freq: ", rebalance_freq)
  }

  min_reb_date <- start_date %m+% lubridate::years(lookback_years)
  reb <- reb[reb >= min_reb_date & reb <= end_date]
  reb
}

# Select symbols from screener result according to config
af_bt_select_symbols <- function(screener_res, screener_config) {
  if (is.null(screener_res) || !nrow(screener_res)) return(character(0))
  if (!"score" %in% names(screener_res)) {
    stop("screener_res must have a 'score' column.")
  }

  res <- data.table::copy(screener_res)
  data.table::setorder(res, -score)

  # Per-type selection if requested and asset_type available
  top_by_type <- screener_config$top_n_by_type
  if (!is.null(top_by_type) && "asset_type" %in% names(res)) {
    sel <- character(0)
    for (tp in names(top_by_type)) {
      k <- as.integer(top_by_type[[tp]])
      if (is.na(k) || k <= 0) next
      tmp <- res[asset_type == tp][1:k, symbol]
      sel <- c(sel, tmp)
    }
    return(unique(sel))
  }

  # Simple Top-N
  top_n <- screener_config$top_n
  if (is.null(top_n) || top_n <= 0) {
    top_n <- min(20L, nrow(res))
  } else {
    top_n <- min(as.integer(top_n), nrow(res))
  }

  res[1:top_n, symbol]
}

# Compute portfolio-level performance statistics
af_bt_compute_stats <- function(equity_dt, returns_dt) {
  eq    <- equity_dt$equity
  dates <- equity_dt$refdate
  if (length(eq) < 2L) {
    return(list())
  }

  # Prefer the logged portfolio returns if available
  r <- returns_dt$port_ret
  r <- r[!is.na(r)]
  if (length(r) < 2L) {
    # fallback: derive from equity
    r <- diff(eq) / head(eq, -1L)
  }

  n_days <- length(r)
  if (n_days < 2L) {
    return(list())
  }

  horizon_years <- as.numeric(difftime(tail(dates, 1L),
                                       head(dates, 1L),
                                       units = "days")) / 365.25
  horizon_years <- max(horizon_years, 1e-9)

  # CAGR from equity
  cagr <- (tail(eq, 1L) / head(eq, 1L))^(1 / horizon_years) - 1

  # Annualized mean/vol (assuming daily freq)
  mean_daily <- mean(r, na.rm = TRUE)
  sd_daily   <- stats::sd(r, na.rm = TRUE)
  ann_return <- mean_daily * 252
  ann_vol    <- sd_daily * sqrt(252)
  sharpe     <- if (ann_vol > 0) ann_return / ann_vol else NA_real_

  # Drawdown
  cummax_eq <- cummax(eq)
  dd <- eq / cummax_eq - 1
  max_dd <- min(dd, na.rm = TRUE)

  # Ulcer index (using fractional drawdown)
  ulcer <- sqrt(mean((dd)^2, na.rm = TRUE))

  list(
    start_date    = head(dates, 1L),
    end_date      = tail(dates, 1L),
    cagr          = cagr,
    ann_return    = ann_return,
    ann_vol       = ann_vol,
    sharpe        = sharpe,
    max_drawdown  = max_dd,
    ulcer_index   = ulcer
  )
}

# Main backtest function
af_backtest <- function(panel,
                        screener_config,
                        risk_config,
                        port_config,
                        rebalance_freq = "monthly",
                        lookback_years = 3L,
                        start_date = NULL,
                        end_date   = NULL) {
  stopifnot("symbol"  %in% names(panel),
            "refdate" %in% names(panel))

  dt <- data.table::copy(panel)
  dt[, refdate := as.Date(refdate)]

  # Use excess_ret_simple if present, else ret_simple
  if ("excess_ret_simple" %in% names(dt)) {
    ret_col <- "excess_ret_simple"
  } else if ("ret_simple" %in% names(dt)) {
    ret_col <- "ret_simple"
  } else {
    stop("Panel must contain 'excess_ret_simple' or 'ret_simple'.")
  }

  all_dates <- sort(unique(dt$refdate))
  if (is.null(start_date)) start_date <- min(all_dates)
  if (is.null(end_date))   end_date   <- max(all_dates)
  start_date <- as.Date(start_date)
  end_date   <- as.Date(end_date)

  # Keep enough history for initial lookback
  dt <- dt[refdate >= (start_date - lubridate::years(lookback_years)) &
           refdate <= end_date]

  # Dates actually used for equity curve
  dates_bt <- sort(unique(dt[refdate >= start_date & refdate <= end_date, refdate]))
  nD <- length(dates_bt)
  if (nD < 2L) {
    stop("Not enough dates in backtest window.")
  }

  reb_dates <- af_bt_compute_rebalance_dates(
    panel         = dt,
    start_date    = start_date,
    end_date      = end_date,
    rebalance_freq = rebalance_freq,
    lookback_years = lookback_years
  )
  if (!length(reb_dates)) {
    stop("No rebalance dates found. Check lookback_years and date range.")
  }

  # Initialize equity and logs
  equity <- rep(NA_real_, nD)
  names(equity) <- as.character(dates_bt)
  equity[1L] <- 1

  returns_log <- data.table::data.table(
    refdate = dates_bt,
    port_ret = NA_real_
  )
  weights_log <- list()

  current_weights <- NULL

  for (k in seq_along(reb_dates)) {
    t_reb <- reb_dates[k]

    # Lookback window for screener/risk
    window_start <- t_reb %m-% lubridate::years(lookback_years)
    panel_window <- dt[refdate >= window_start & refdate <= t_reb]

    # 1) Screener on full universe in the window
    scr_t <- af_run_screener(
      panel      = panel_window,
      config     = screener_config,
      as_of_date = t_reb
    )

    selected <- af_bt_select_symbols(scr_t, screener_config)
    if (!length(selected)) {
      warning(sprintf("No symbols selected at rebalance %s. Keeping previous weights.",
                      as.character(t_reb)))
      if (is.null(current_weights)) next
    } else {
      # 2) Risk model on selected subset
      extra_args <- risk_config$extra_args
      if (is.null(extra_args)) extra_args <- list()
      risk_t <- do.call(
        af_risk_build,
        c(
          list(
            panel        = panel_window,
            symbols      = selected,
            end_date     = t_reb,
            window_years = risk_config$window_years,
            cov_method   = risk_config$cov_method,
            mu_method    = risk_config$mu_method
          ),
          extra_args
        )
      )

      # 3) Portfolio optimization
      port_t <- af_build_portfolio(
        mu     = risk_t$mu,
        Sigma  = risk_t$Sigma,
        config = port_config
      )
      current_weights <- port_t$weights

      weights_log[[as.character(t_reb)]] <- data.table::data.table(
        refdate = t_reb,
        symbol  = names(current_weights),
        weight  = as.numeric(current_weights)
      )
    }

    # 4) Apply these weights from next day until next rebalance
    idx_reb <- which(dates_bt == t_reb)
    if (!length(idx_reb)) next  # e.g. t_reb not an actual trading date

    idx_start <- idx_reb + 1L
    if (k < length(reb_dates)) {
      idx_end <- which(dates_bt == reb_dates[k + 1L])
    } else {
      idx_end <- nD
    }
    if (idx_start > idx_end) next
    if (is.null(current_weights)) next  # no portfolio yet

    for (i in idx_start:idx_end) {
      d <- dates_bt[i]
      day_ret <- dt[refdate == d & symbol %in% names(current_weights),
                    .(symbol, ret = get(ret_col))]

      if (nrow(day_ret) == 0L) {
        port_ret_d <- 0
      } else {
        data.table::setkey(day_ret, symbol)
        w_vec   <- current_weights[day_ret$symbol]
        ret_vec <- day_ret$ret
        port_ret_d <- sum(w_vec * ret_vec, na.rm = TRUE)
      }

      returns_log[refdate == d, port_ret := port_ret_d]

      if (i == 1L) {
        equity[i] <- 1 * (1 + port_ret_d)
      } else {
        if (is.na(equity[i - 1L])) {
          equity[i - 1L] <- equity[i - 2L]
        }
        equity[i] <- equity[i - 1L] * (1 + port_ret_d)
      }
    }
  }

  # Fill any remaining NAs in equity curve (e.g. before first rebalance)
  equity <- zoo::na.locf(equity, fromLast = FALSE, na.rm = FALSE)
  equity_dt <- data.table::data.table(
    refdate = dates_bt,
    equity  = equity
  )

  weights_dt <- if (length(weights_log)) {
    data.table::rbindlist(weights_log, use.names = TRUE, fill = TRUE)
  } else {
    data.table::data.table(refdate = as.Date(NA),
                           symbol  = NA_character_,
                           weight  = NA_real_)
  }

  stats <- af_bt_compute_stats(equity_dt, returns_log)

  list(
    equity_curve    = equity_dt,
    returns         = returns_log,
    weights         = weights_dt,
    rebalance_dates = reb_dates,
    stats           = stats
  )
}



###############################################################################
### FILE: autofinance_config.R
###############################################################################
############################################################
# autofinance_config.R
# Global config: DB path, package management
############################################################

# ---- 1. DB PATH ----

af_default_db_path <- "data/autofinance.sqlite"

af_get_db_path <- function() {
  # Ensure ./data exists
  if (!dir.exists("data")) dir.create("data", recursive = TRUE)
  af_default_db_path
}

# Constant used by all other modules
AF_DB_PATH <- af_get_db_path()

# ---- 2. DEFAULT PACKAGES ----
# This is "everything we conceptually care about" for the project.
# Not all of them are required by every script, but af_attach_packages()
# will be called with specific subsets where needed.

af_default_packages <- c(
  # Core infra
  "data.table",
  "DBI",
  "RSQLite",

  # Time series & finance
  "xts",
  "zoo",
  "quantmod",
  "TTR",

  # Utils
  "httr",
  "jsonlite",
  "lubridate",

  # Optimization
  "quadprog",

  # Risk-model packages (we DO care about them)
  # These may be heavy; they will be installed on demand.
  "rugarch",
  "rmgarch",
  "vars"
)

# ---- 3. PACKAGE ATTACH / INSTALL LOGIC ----

af_attach_packages <- function(pkgs = af_default_packages,
                               auto_install = TRUE) {
  pkgs <- unique(pkgs)

  # Ensure we have a CRAN repo set (needed for install.packages)
  if (auto_install) {
    repos <- getOption("repos")
    if (is.null(repos) || identical(repos["CRAN"], "@CRAN@") || is.na(repos["CRAN"])) {
      options(repos = c(CRAN = "https://cloud.r-project.org"))
    }
  }

  for (p in pkgs) {
    if (!requireNamespace(p, quietly = TRUE)) {
      if (auto_install) {
        message("Package '", p, "' not found. Installing from CRAN...")
        utils::install.packages(p)
      }
      # Try again
      if (!requireNamespace(p, quietly = TRUE)) {
        stop(
          sprintf("Package '%s' is required but could not be loaded/installed.", p),
          call. = FALSE
        )
      }
    }
    # Attach so you can inspect stuff interactively if you want
    suppressPackageStartupMessages(
      library(p, character.only = TRUE)
    )
  }

  invisible(TRUE)
}



###############################################################################
### FILE: autofinance_db_core.R
###############################################################################
############################################################
# autofinance_db_core.R
############################################################

af_db_connect <- function(db_path = AF_DB_PATH) {
  af_attach_packages(c("DBI", "RSQLite"))
  con <- RSQLite::dbConnect(RSQLite::SQLite(), db_path)
  DBI::dbExecute(con, "PRAGMA journal_mode = WAL;")
  DBI::dbExecute(con, "PRAGMA synchronous = NORMAL;")
  DBI::dbExecute(con, "PRAGMA foreign_keys = ON;")
  con
}

af_db_disconnect <- function(con) {
  if (!is.null(con) && DBI::dbIsValid(con)) {
    DBI::dbDisconnect(con)
  }
  invisible(TRUE)
}

af_db_init <- function(con = af_db_connect()) {
  on.exit(af_db_disconnect(con), add = TRUE)
  af_attach_packages("DBI")

  # assets_meta
  DBI::dbExecute(con, "
    CREATE TABLE IF NOT EXISTS assets_meta (
      symbol TEXT PRIMARY KEY,
      asset_type TEXT,
      sector TEXT,
      active INTEGER,
      last_update_splits TEXT,
      last_update_divs TEXT
    )
  ")

  # prices_raw = fita B3
  DBI::dbExecute(con, "
    CREATE TABLE IF NOT EXISTS prices_raw (
      symbol TEXT,
      refdate TEXT,
      open REAL,
      high REAL,
      low REAL,
      close REAL,
      vol_fin REAL,
      qty REAL,
      PRIMARY KEY (symbol, refdate)
    ) WITHOUT ROWID
  ")

  # adjustments = corporate actions da Yahoo
  DBI::dbExecute(con, "
    CREATE TABLE IF NOT EXISTS adjustments (
      symbol TEXT,
      date TEXT,
      type TEXT,   -- 'SPLIT' ou 'DIVIDEND'
      value REAL,
      PRIMARY KEY (symbol, date, type)
    ) WITHOUT ROWID
  ")

  # macro_series = SGS / IBOV / USD etc.
  DBI::dbExecute(con, "
    CREATE TABLE IF NOT EXISTS macro_series (
      series_id TEXT,
      refdate   TEXT,
      value     REAL,
      PRIMARY KEY (series_id, refdate)
    ) WITHOUT ROWID
  ")

  invisible(TRUE)
}

af_db_insert_prices_raw <- function(con, dt) {
  af_attach_packages(c("DBI", "data.table"))
  dt <- data.table::as.data.table(dt)
  if (!nrow(dt)) return(invisible(TRUE))

  # Normalizar colunas
  if (!"refdate" %in% names(dt)) {
    stop("af_db_insert_prices_raw: 'refdate' column missing.")
  }
  dt[, refdate := as.character(as.Date(refdate))]

  cols <- c("symbol", "refdate", "open", "high", "low", "close", "vol_fin", "qty")
  missing <- setdiff(cols, names(dt))
  if (length(missing) > 0L) {
    stop("af_db_insert_prices_raw: missing columns: ", paste(missing, collapse = ", "))
  }

  sql <- "
    INSERT OR REPLACE INTO prices_raw
    (symbol, refdate, open, high, low, close, vol_fin, qty)
    VALUES (:symbol, :refdate, :open, :high, :low, :close, :vol_fin, :qty)
  "

  DBI::dbBegin(con)
  DBI::dbExecute(con, sql, params = dt[, ..cols])
  DBI::dbCommit(con)
  invisible(TRUE)
}

af_db_insert_adjustments <- function(con, dt) {
  af_attach_packages(c("DBI", "data.table"))
  dt <- data.table::as.data.table(dt)
  if (!nrow(dt)) return(invisible(TRUE))

  required <- c("symbol", "date", "type", "value")
  missing  <- setdiff(required, names(dt))
  if (length(missing) > 0L) {
    stop("af_db_insert_adjustments: missing columns: ", paste(missing, collapse = ", "))
  }

  dt[, date := as.character(as.Date(date))]

  sql <- "
    INSERT OR REPLACE INTO adjustments
    (symbol, date, type, value)
    VALUES (:symbol, :date, :type, :value)
  "

  DBI::dbBegin(con)
  DBI::dbExecute(con, sql, params = dt[, ..required])
  DBI::dbCommit(con)
  invisible(TRUE)
}

af_db_insert_macro_series <- function(con, dt) {
  af_attach_packages(c("DBI", "data.table"))
  dt <- data.table::as.data.table(dt)
  if (!nrow(dt)) return(invisible(TRUE))

  required <- c("series_id", "refdate", "value")
  missing  <- setdiff(required, names(dt))
  if (length(missing) > 0L) {
    stop("af_db_insert_macro_series: missing columns: ", paste(missing, collapse = ", "))
  }

  dt[, refdate := as.character(as.Date(refdate))]

  sql <- "
    INSERT OR REPLACE INTO macro_series
    (series_id, refdate, value)
    VALUES (:series_id, :refdate, :value)
  "

  DBI::dbBegin(con)
  DBI::dbExecute(con, sql, params = dt[, ..required])
  DBI::dbCommit(con)
  invisible(TRUE)
}

af_db_upsert_assets_meta <- function(con, dt) {
  af_attach_packages(c("DBI", "data.table"))
  dt <- data.table::as.data.table(dt)
  if (!nrow(dt)) return(invisible(TRUE))

  cols <- c("symbol", "asset_type", "sector", "active",
            "last_update_splits", "last_update_divs")
  missing <- setdiff(cols, names(dt))
  if (length(missing) > 0L) {
    stop("af_db_upsert_assets_meta: missing columns: ", paste(missing, collapse = ", "))
  }

  sql <- "
    INSERT OR REPLACE INTO assets_meta
    (symbol, asset_type, sector, active, last_update_splits, last_update_divs)
    VALUES (:symbol, :asset_type, :sector, :active, :last_update_splits, :last_update_divs)
  "

  DBI::dbBegin(con)
  DBI::dbExecute(con, sql, params = dt[, ..cols])
  DBI::dbCommit(con)
  invisible(TRUE)
}

af_db_get_symbols <- function(con) {
  af_attach_packages(c("DBI", "data.table"))
  res <- DBI::dbGetQuery(con, "SELECT DISTINCT symbol FROM prices_raw")
  data.table::as.data.table(res)$symbol
}



###############################################################################
### FILE: autofinance_fiscal_trading.R
###############################################################################
############################################################
# autofinance_fiscal_trading.R
# Impostos e custos sobre trades/posições (camada net-of-tax)
############################################################

# AQUI é extremamente simplificado e serve como esqueleto.
# Você vai precisar ajustar para refletir as regras exatas BR (20k isenção, etc).

af_fiscal_rules_default <- list(
  tax_rate_equity = 0.15,   # ganho de capital
  tax_rate_bdr    = 0.15,
  tax_rate_fii    = 0.20,   # GC 20% (exemplo)
  trade_cost_bp   = 0.0005  # custo proporcional (0.05%)
)

af_apply_fiscal_trading <- function(trades,
                                    prices_panel,
                                    assets_meta,
                                    rules = af_fiscal_rules_default) {
  af_attach_packages("data.table")
  tr <- data.table::as.data.table(trades)
  pr <- data.table::as.data.table(prices_panel)
  am <- data.table::as.data.table(assets_meta)

  # Simplificação: supõe que trades$weight é peso alvo; converte para valor nominal
  # com base num patrimônio hipotético inicial de 1.
  # Você pode reimplementar para trabalhar com quantidades reais e preços.

  # Aqui, apenas calculamos custo de transação aproximado:
  # custo = sum(|Δw|) * equity * trade_cost_bp ao longo do tempo.
  # Imposto: placeholder sem cálculo detalhado de GC.

  tr <- am[tr, on = .(symbol), nomatch = 0L]

  # custo por trade: weight * trade_cost_bp
  tr[, trade_cost := abs(weight) * rules$trade_cost_bp]

  total_trade_cost <- sum(tr$trade_cost, na.rm = TRUE)

  list(
    total_trade_cost = total_trade_cost,
    # espaço para somar imposto de GC real:
    total_tax = NA_real_
  )
}



###############################################################################
### FILE: autofinance_ingest_b3.R
###############################################################################
############################################################
# autofinance_ingest_b3.R
# Sincronização COTAHIST -> prices_raw
############################################################

# ATENÇÃO:
# Aqui eu NÃO chuto a API do rb3, para não te ferrar com função errada.
# Você pluga seu fetch com rb3 dentro de `af_fetch_cotahist_year()`.

af_fetch_cotahist_year <- function(year) {
  # TODO: implemente usando rb3::... com sua pipeline atual.
  # Deve retornar data.table com colunas:
  # symbol, refdate (Date), open, high, low, close, vol_fin, qty
  stop("Implemente af_fetch_cotahist_year(year) usando rb3 na sua máquina.")
}

af_sync_b3 <- function(con = af_db_connect(),
                       years = NULL,
                       verbose = TRUE) {
  on.exit(af_db_disconnect(con), add = TRUE)
  if (!requireNamespace("data.table", quietly = TRUE)) {
    stop("Package 'data.table' required.")
  }
  data.table::setDTthreads(1L)

  if (is.null(years)) {
    max_ref <- af_db_get_max_refdate_prices(con)
    if (is.na(max_ref)) {
      # DB vazio -> pega últimos N anos ou toda a série, a seu critério
      current_year <- as.integer(format(Sys.Date(), "%Y"))
      years <- current_year
    } else {
      last_year <- as.integer(substr(max_ref, 1, 4))
      current_year <- as.integer(format(Sys.Date(), "%Y"))
      years <- seq.int(last_year, current_year)
    }
  }

  for (y in years) {
    if (verbose) message("af_sync_b3: fetching COTAHIST year ", y)
    dt <- af_fetch_cotahist_year(y)
    if (!inherits(dt, "data.table")) data.table::setDT(dt)
    if (!all(c("symbol", "refdate", "open", "high", "low", "close", "vol_fin", "qty") %in% names(dt))) {
      stop("af_fetch_cotahist_year must return symbol, refdate, open, high, low, close, vol_fin, qty")
    }
    af_db_insert_prices_raw(con, dt)
  }

  invisible(TRUE)
}



###############################################################################
### FILE: autofinance_ingest_corporate.R
###############################################################################
############################################################
# autofinance_ingest_corporate.R
# Splits & dividendos via quantmod -> adjustments
############################################################

af_symbol_to_yahoo <- function(symbol) {
  paste0(symbol, ".SA")
}

af_yahoo_to_symbol <- function(yahoo_symbol) {
  sub("\\.SA$", "", yahoo_symbol)
}

af_sync_yahoo_splits <- function(con = af_db_connect(),
                                 symbols = NULL,
                                 from_default = as.Date("2000-01-01"),
                                 verbose = TRUE) {
  on.exit(af_db_disconnect(con), add = TRUE)
  af_attach_packages(c("DBI", "data.table", "quantmod"))

  af_db_init(con)

  meta <- data.table::as.data.table(
    DBI::dbGetQuery(con, "SELECT symbol, last_update_splits FROM assets_meta")
  )

  if (!is.null(symbols)) {
    meta <- meta[symbol %in% symbols]
  }

  if (!nrow(meta)) {
    stop("af_sync_yahoo_splits: no symbols in assets_meta.")
  }

  results <- list()

  for (i in seq_len(nrow(meta))) {
    sym <- meta$symbol[i]
    last_upd <- meta$last_update_splits[i]
    ysym <- af_symbol_to_yahoo(sym)

    from_date <- if (is.na(last_upd) || last_upd == "" || is.null(last_upd)) {
      from_default
    } else {
      as.Date(last_upd) - 5L  # pequena folga
    }

    if (verbose) message("Splits: ", sym, " (", ysym, ") from ", from_date, "...")

    sp <- tryCatch(
      quantmod::getSplits(ysym, from = from_date),
      error = function(e) NULL
    )
    if (is.null(sp) || nrow(sp) == 0L) {
      # mesmo sem splits, atualizamos last_update_splits
      new_last <- format(Sys.Date(), "%Y-%m-%d")
      DBI::dbExecute(
        con,
        "UPDATE assets_meta SET last_update_splits = ? WHERE symbol = ?",
        params = list(new_last, sym)
      )
      next
    }

    dt_sp <- data.table::data.table(
      symbol = sym,
      date   = as.Date(index(sp)),
      type   = "SPLIT",
      value  = as.numeric(sp[, 1])
    )
    results[[sym]] <- dt_sp

    new_last <- format(max(dt_sp$date), "%Y-%m-%d")
    DBI::dbExecute(
      con,
      "UPDATE assets_meta SET last_update_splits = ? WHERE symbol = ?",
      params = list(new_last, sym)
    )
  }

  if (length(results) > 0L) {
    dt_all <- data.table::rbindlist(results, fill = TRUE)
    af_db_insert_adjustments(con, dt_all)
  }

  invisible(TRUE)
}

af_sync_yahoo_dividends <- function(con = af_db_connect(),
                                    symbols = NULL,
                                    from_default = as.Date("2000-01-01"),
                                    verbose = TRUE) {
  on.exit(af_db_disconnect(con), add = TRUE)
  af_attach_packages(c("DBI", "data.table", "quantmod"))

  af_db_init(con)

  meta <- data.table::as.data.table(
    DBI::dbGetQuery(con, "SELECT symbol, last_update_divs FROM assets_meta")
  )

  if (!is.null(symbols)) {
    meta <- meta[symbol %in% symbols]
  }

  if (!nrow(meta)) {
    stop("af_sync_yahoo_dividends: no symbols in assets_meta.")
  }

  results <- list()

  for (i in seq_len(nrow(meta))) {
    sym <- meta$symbol[i]
    last_upd <- meta$last_update_divs[i]
    ysym <- af_symbol_to_yahoo(sym)

    from_date <- if (is.na(last_upd) || last_upd == "" || is.null(last_upd)) {
      from_default
    } else {
      as.Date(last_upd) - 5L
    }

    if (verbose) message("Dividends: ", sym, " (", ysym, ") from ", from_date, "...")

    dv <- tryCatch(
      quantmod::getDividends(ysym, from = from_date),
      error = function(e) NULL
    )
    if (is.null(dv) || nrow(dv) == 0L) {
      new_last <- format(Sys.Date(), "%Y-%m-%d")
      DBI::dbExecute(
        con,
        "UPDATE assets_meta SET last_update_divs = ? WHERE symbol = ?",
        params = list(new_last, sym)
      )
      next
    }

    dt_dv <- data.table::data.table(
      symbol = sym,
      date   = as.Date(index(dv)),
      type   = "DIVIDEND",
      value  = as.numeric(dv[, 1])
    )
    results[[sym]] <- dt_dv

    new_last <- format(max(dt_dv$date), "%Y-%m-%d")
    DBI::dbExecute(
      con,
      "UPDATE assets_meta SET last_update_divs = ? WHERE symbol = ?",
      params = list(new_last, sym)
    )
  }

  if (length(results) > 0L) {
    dt_all <- data.table::rbindlist(results, fill = TRUE)
    af_db_insert_adjustments(con, dt_all)
  }

  invisible(TRUE)
}



###############################################################################
### FILE: autofinance_ingest_macro.R
###############################################################################
############################################################
# autofinance_ingest_macro.R
# BCB SGS -> macro_series
############################################################

af_fetch_sgs_series <- function(series_id,
                                start_date,
                                end_date) {
  af_attach_packages(c("httr", "jsonlite", "data.table"))

  start_str <- format(as.Date(start_date), "%d/%m/%Y")
  end_str   <- format(as.Date(end_date),   "%d/%m/%Y")

  url <- sprintf(
    "https://api.bcb.gov.br/dados/serie/bcdata.sgs.%d/dados?formato=json&dataInicial=%s&dataFinal=%s",
    as.integer(series_id),
    start_str,
    end_str
  )

  resp <- httr::GET(url)
  if (httr::http_error(resp)) {
    stop("af_fetch_sgs_series: HTTP error for series ", series_id)
  }

  txt <- httr::content(resp, as = "text", encoding = "UTF-8")
  js  <- jsonlite::fromJSON(txt, simplifyDataFrame = TRUE)
  dt  <- data.table::as.data.table(js)
  if (!nrow(dt)) {
    return(data.table::data.table(
      series_id = integer(0),
      refdate   = as.Date(character(0)),
      value     = numeric(0)
    ))
  }

  # campos padrão: data, valor
  dt[, refdate := as.Date(data, format = "%d/%m/%Y")]
  dt[, value   := as.numeric(gsub(",", ".", valor))]

  out <- dt[, .(series_id = as.integer(series_id),
                refdate,
                value)]
  out
}

af_sync_macro_series <- function(con = af_db_connect(),
                                 series_ids,
                                 start_date,
                                 end_date,
                                 overwrite = FALSE,
                                 verbose = TRUE) {
  on.exit(af_db_disconnect(con), add = TRUE)
  af_db_init(con)
  af_attach_packages(c("DBI", "data.table"))

  series_ids <- unique(as.integer(series_ids))

  for (sid in series_ids) {
    if (verbose) message("SGS series ", sid, " from ", start_date, " to ", end_date, "...")
    dt <- af_fetch_sgs_series(sid, start_date, end_date)
    if (!nrow(dt)) {
      if (verbose) message("  no data for series ", sid)
      next
    }
    af_db_insert_macro_series(con, dt)
  }

  invisible(TRUE)
}

af_get_macro_series <- function(con,
                                series_ids,
                                start_date,
                                end_date) {
  af_attach_packages(c("DBI", "data.table"))
  series_ids <- unique(as.character(series_ids))

  q <- sprintf("
    SELECT series_id, refdate, value
    FROM macro_series
    WHERE series_id IN (%s)
      AND refdate >= '%s'
      AND refdate <= '%s'
  ",
    paste(sprintf("'%s'", series_ids), collapse = ","),
    format(as.Date(start_date), "%Y-%m-%d"),
    format(as.Date(end_date),   "%Y-%m-%d")
  )

  dt <- data.table::as.data.table(DBI::dbGetQuery(con, q))
  if (!nrow(dt)) return(dt)
  dt[, refdate := as.Date(refdate)]
  dt
}



###############################################################################
### FILE: autofinance_ingest_splits.R
###############################################################################
############################################################
# autofinance_ingest_splits.R
# Sincronização de splits (e futuramente dividendos) via quantmod
############################################################

af_sync_splits <- function(con = af_db_connect(),
                           symbols = NULL,
                           max_age_days = 7L,
                           force = FALSE,
                           verbose = TRUE) {
  on.exit(af_db_disconnect(con), add = TRUE)
  af_attach_packages(c("data.table", "quantmod"))
  dt_assets <- data.table::as.data.table(
    DBI::dbGetQuery(con, "SELECT symbol, active, last_update_splits FROM assets_meta")
  )

  if (!is.null(symbols)) {
    dt_assets <- dt_assets[symbol %in% symbols]
  } else {
    dt_assets <- dt_assets[active == 1L]
  }

  today <- Sys.Date()
  dt_assets[, last_date := as.Date(last_update_splits)]
  if (!force) {
    dt_assets <- dt_assets[is.na(last_date) | (today - last_date) > max_age_days]
  }

  if (nrow(dt_assets) == 0L) {
    if (verbose) message("af_sync_splits: nothing to update.")
    return(invisible(TRUE))
  }

  all_symbols_checked <- character(0)

  for (sym in dt_assets$symbol) {
    ysym <- paste0(sym, ".SA")
    if (verbose) message("af_sync_splits: ", sym, " (", ysym, ")")
    splits_xts <- NULL
    try({
      suppressWarnings(
        splits_xts <- quantmod::getSplits(ysym, auto.assign = FALSE)
      )
    }, silent = TRUE)

    if (!is.null(splits_xts) && NROW(splits_xts) > 0) {
      dt_s <- data.table::data.table(
        symbol = sym,
        date   = as.Date(zoo::index(splits_xts)),
        type   = "SPLIT",
        value  = as.numeric(splits_xts[, 1])
      )
      af_db_insert_corporate_actions(con, dt_s)
    }

    # Atualiza metadata para não ficar rechecando
    DBI::dbExecute(
      con,
      "INSERT OR REPLACE INTO assets_meta
       (symbol, asset_type, sector, active, last_update_splits, last_update_divs)
       VALUES (
         :symbol,
         COALESCE((SELECT asset_type FROM assets_meta WHERE symbol = :symbol), NULL),
         COALESCE((SELECT sector     FROM assets_meta WHERE symbol = :symbol), NULL),
         COALESCE((SELECT active     FROM assets_meta WHERE symbol = :symbol), 1),
         :last_update_splits,
         COALESCE((SELECT last_update_divs FROM assets_meta WHERE symbol = :symbol), NULL)
       )",
      params = list(
        symbol = sym,
        last_update_splits = as.character(today)
      )
    )

    all_symbols_checked <- c(all_symbols_checked, sym)
    Sys.sleep(0.2) # educado com Yahoo
  }

  invisible(all_symbols_checked)
}



###############################################################################
### FILE: autofinance_panel.R
###############################################################################
############################################################
# autofinance_panel.R
#
# Painel de preços ajustados + retornos
#
# Depende de:
#   - autofinance_config.R (AF_DB_PATH, af_attach_packages)
#   - Banco SQLite com:
#       prices_raw(symbol, refdate, open, high, low, close, vol_fin, qty)
#       adjustments(symbol, date, type, value)  -- pelo menos type == "SPLIT"
#
# Principais funções:
#   - af_load_prices_raw()
#   - af_load_adjustments()
#   - af_build_adjusted_panel()
#   - af_compute_returns()
############################################################

if (!exists("af_attach_packages")) {
  if (file.exists("autofinance_config.R")) {
    source("autofinance_config.R")
  } else {
    stop("autofinance_config.R not found; please source it before using panel.")
  }
}

af_attach_packages(c("data.table", "DBI", "RSQLite", "quantmod", "xts", "TTR"))

# ----------------------------------------------------------------------
# Helper: small internal connection helper (optional)
# Você pode continuar usando seu próprio af_db_connect(), se já existir.
# Aqui deixo um helper leve para casos em que você só tem AF_DB_PATH.
# ----------------------------------------------------------------------

af_panel_db_connect <- function(db_path = AF_DB_PATH) {
  RSQLite::dbConnect(RSQLite::SQLite(), db_path)
}

# ----------------------------------------------------------------------
# 1) Carregar preços crus (prices_raw) do DB
# ----------------------------------------------------------------------

# Retorna data.table:
#   symbol, refdate(Date), open, high, low, close, vol_fin, qty
af_load_prices_raw <- function(con,
                               symbols    = NULL,
                               start_date = NULL,
                               end_date   = NULL) {
  if (!DBI::dbIsValid(con)) {
    stop("af_load_prices_raw: 'con' is not a valid DBI connection.")
  }

  where_clauses <- c()

  if (!is.null(symbols) && length(symbols) > 0) {
    syms_sql <- paste(DBI::dbQuoteString(con, symbols), collapse = ",")
    where_clauses <- c(where_clauses, sprintf("symbol IN (%s)", syms_sql))
  }

  if (!is.null(start_date)) {
    start_date <- as.character(as.Date(start_date))
    where_clauses <- c(where_clauses, sprintf("refdate >= '%s'", start_date))
  }

  if (!is.null(end_date)) {
    end_date <- as.character(as.Date(end_date))
    where_clauses <- c(where_clauses, sprintf("refdate <= '%s'", end_date))
  }

  where_sql <- if (length(where_clauses) > 0) {
    paste("WHERE", paste(where_clauses, collapse = " AND "))
  } else {
    ""
  }

  sql <- sprintf("
    SELECT symbol, refdate, open, high, low, close, vol_fin, qty
    FROM prices_raw
    %s
    ORDER BY symbol, refdate
  ", where_sql)

  dt <- data.table::as.data.table(DBI::dbGetQuery(con, sql))
  if (nrow(dt) == 0L) {
    warning("af_load_prices_raw: no rows returned for given filters.")
    return(dt)
  }

  dt[, refdate := as.Date(refdate)]
  dt[]
}

# ----------------------------------------------------------------------
# 2) Carregar ajustes (splits/dividends) do DB
# ----------------------------------------------------------------------

# Retorna data.table:
#   symbol, date(Date), type, value
af_load_adjustments <- function(con,
                                symbols    = NULL,
                                start_date = NULL,
                                end_date   = NULL,
                                types      = c("SPLIT")) {
  if (!DBI::dbIsValid(con)) {
    stop("af_load_adjustments: 'con' is not a valid DBI connection.")
  }

  where_clauses <- c()

  if (!is.null(symbols) && length(symbols) > 0) {
    syms_sql <- paste(DBI::dbQuoteString(con, symbols), collapse = ",")
    where_clauses <- c(where_clauses, sprintf("symbol IN (%s)", syms_sql))
  }

  if (!is.null(start_date)) {
    start_date <- as.character(as.Date(start_date))
    where_clauses <- c(where_clauses, sprintf("date >= '%s'", start_date))
  }

  if (!is.null(end_date)) {
    end_date <- as.character(as.Date(end_date))
    where_clauses <- c(where_clauses, sprintf("date <= '%s'", end_date))
  }

  if (!is.null(types) && length(types) > 0) {
    tps_sql <- paste(DBI::dbQuoteString(con, types), collapse = ",")
    where_clauses <- c(where_clauses, sprintf("type IN (%s)", tps_sql))
  }

  where_sql <- if (length(where_clauses) > 0) {
    paste("WHERE", paste(where_clauses, collapse = " AND "))
  } else {
    ""
  }

  sql <- sprintf("
    SELECT symbol, date, type, value
    FROM adjustments
    %s
    ORDER BY symbol, date
  ", where_sql)

  dt <- data.table::as.data.table(DBI::dbGetQuery(con, sql))
  if (nrow(dt) == 0L) {
    # não é erro: pode simplesmente não haver splits no período
    dt[, date := as.Date(character())]
    return(dt)
  }

  dt[, date := as.Date(date)]
  dt[]
}

# ----------------------------------------------------------------------
# 3) Aplicar splits via quantmod::adjustOHLC (por símbolo)
# ----------------------------------------------------------------------

# prices_dt: data.table para UM símbolo:
#   refdate, open, high, low, close
# splits_dt: data.table para UM símbolo:
#   date, value (type=="SPLIT")
#
# Retorna data.table com colunas:
#   refdate, open_adj, high_adj, low_adj, close_adj
af_apply_splits_one_symbol <- function(prices_dt,
                                       splits_dt = NULL) {
  if (nrow(prices_dt) == 0L) return(NULL)

  # Garantir ordenação
  data.table::setorder(prices_dt, refdate)

  # Construir xts OHLC
  ohlc_mat <- as.matrix(prices_dt[, .(open, high, low, close)])
  colnames(ohlc_mat) <- c("Open", "High", "Low", "Close")
  ohlc_xts <- xts::xts(ohlc_mat, order.by = prices_dt$refdate)

  split_xts <- NULL
  if (!is.null(splits_dt) && nrow(splits_dt) > 0L) {
    # Quantmod espera um objeto tipo xts:
    #   índice = datas, valores = ratio (como getSplits() retorna)
    split_xts <- xts::xts(splits_dt$value,
                          order.by = splits_dt$date)
  }

  # Ajuste
  # use.Adjusted = FALSE -> gera colunas ajustadas a partir de OHLC
  if (!is.null(split_xts)) {
    ohlc_adj <- quantmod::adjustOHLC(
      ohlc_xts,
      use.Adjusted = FALSE,
      split = split_xts,
      div   = NULL
    )
  } else {
    # Sem splits: apenas replica
    ohlc_adj <- ohlc_xts
  }

  out <- data.table::data.table(
    refdate   = as.Date(zoo::index(ohlc_adj)),
    open_adj  = as.numeric(ohlc_adj[, "Open"]),
    high_adj  = as.numeric(ohlc_adj[, "High"]),
    low_adj   = as.numeric(ohlc_adj[, "Low"]),
    close_adj = as.numeric(ohlc_adj[, "Close"])
  )

  out[]
}

# ----------------------------------------------------------------------
# 4) Construir painel ajustado para vários símbolos
# ----------------------------------------------------------------------

# Retorna data.table:
#   symbol, refdate, open_adj, high_adj, low_adj, close_adj,
#   vol_fin, qty
#
# Nota: vol_fin e qty vêm do prices_raw sem ajuste (o padrão mesmo).
#       Se quiser "volume ajustado", dá pra derivar depois.
af_build_adjusted_panel <- function(con,
                                    symbols    = NULL,
                                    start_date = NULL,
                                    end_date   = NULL) {
  # 1) Carregar preços crus
  prices <- af_load_prices_raw(
    con        = con,
    symbols    = symbols,
    start_date = start_date,
    end_date   = end_date
  )
  if (nrow(prices) == 0L) {
    stop("af_build_adjusted_panel: no raw prices found for given filters.")
  }

  # 2) Carregar splits
  #    Usamos um pouco mais largo no início (start_date - small buffer) só por segurança,
  #    mas por simplicidade, aqui usamos o mesmo intervalo.
  adj <- af_load_adjustments(
    con        = con,
    symbols    = unique(prices$symbol),
    start_date = start_date,
    end_date   = end_date,
    types      = c("SPLIT")
  )

  # 3) Aplicar por símbolo
  out_list <- list()
  syms <- sort(unique(prices$symbol))

  for (sym in syms) {
    p_sym <- prices[symbol == sym]
    s_sym <- if (nrow(adj) > 0L) adj[symbol == sym & type == "SPLIT"] else NULL

    adj_sym <- af_apply_splits_one_symbol(
      prices_dt = p_sym[, .(refdate, open, high, low, close)],
      splits_dt = s_sym[, .(date, value)]
    )

    if (!is.null(adj_sym) && nrow(adj_sym) > 0L) {
      # Juntar vol_fin e qty
      merged <- merge(
        p_sym[, .(refdate, vol_fin, qty)],
        adj_sym,
        by = "refdate",
        all.y = TRUE
      )

      merged[, symbol := sym]
      data.table::setcolorder(merged,
                              c("symbol", "refdate",
                                "open_adj", "high_adj", "low_adj", "close_adj",
                                "vol_fin", "qty"))

      out_list[[sym]] <- merged[]
    }
  }

  panel_adj <- data.table::rbindlist(out_list, use.names = TRUE, fill = TRUE)
  data.table::setorder(panel_adj, symbol, refdate)

  panel_adj[]
}

# ----------------------------------------------------------------------
# 5) Calcular retornos (simples e excessos)
# ----------------------------------------------------------------------

# panel_adj: data.table com:
#   symbol, refdate, close_adj (e, opcionalmente, outras colunas)
#
# rf_daily_dt (opcional): data.table com:
#   refdate, rf_daily  (taxa diária em decimal, ex: 0.00025 ~ 0.025%/dia)
#
# Retorna painel com colunas extras:
#   ret_simple, excess_ret_simple (se RF fornecido)
af_compute_returns <- function(panel_adj,
                               rf_daily_dt = NULL,
                               rf_col      = "rf_daily") {
  dt <- data.table::as.data.table(panel_adj)
  if (!all(c("symbol", "refdate", "close_adj") %in% names(dt))) {
    stop("af_compute_returns: panel_adj must contain 'symbol', 'refdate', 'close_adj'.")
  }

  data.table::setorder(dt, symbol, refdate)

  # Retorno simples por ativo
  dt[, close_lag := data.table::shift(close_adj, n = 1L, type = "lag"),
     by = symbol]

  dt[, ret_simple := ifelse(
    !is.na(close_lag) & close_lag != 0,
    (close_adj / close_lag) - 1,
    NA_real_
  )]

  dt[, close_lag := NULL]

  # Se RF fornecido, calcular excesso
  if (!is.null(rf_daily_dt)) {
    rf <- data.table::as.data.table(rf_daily_dt)
    if (!("refdate" %in% names(rf))) {
      stop("rf_daily_dt must contain 'refdate' column.")
    }
    if (!(rf_col %in% names(rf))) {
      stop(sprintf("rf_daily_dt must contain column '%s'.", rf_col))
    }
    rf[, refdate := as.Date(refdate)]

    # merge por data (mesma RF para todos ativos naquele dia)
    dt <- merge(
      dt,
      rf[, .(refdate, rf_daily = get(rf_col))],
      by = "refdate",
      all.x = TRUE
    )

    dt[, excess_ret_simple := ifelse(
      !is.na(ret_simple) & !is.na(rf_daily),
      ret_simple - rf_daily,
      NA_real_
    )]
  }

  dt[]
}



###############################################################################
### FILE: autofinance_portfolio_engine.R
###############################################################################
############################################################
# autofinance_portfolio_engine.R
#
# Pure portfolio construction layer:
#   Input : mu (expected returns), Sigma (covariance matrix),
#           port_config (constraints / mode)
#   Output: weights vector + some metadata
#
# Modes:
#   - "equal"     : equal weight
#   - "inv_vol"   : 1/σ weighting
#   - "min_var"   : minimum-variance QP
#   - "mean_var"  : Markowitz (risk aversion)
#   - "max_sharpe": approximated via QP on (mu - rf)
############################################################

if (!exists("af_attach_packages")) {
  if (file.exists("autofinance_config.R")) {
    source("autofinance_config.R")
  } else {
    stop("autofinance_config.R not found; please source it before portfolio engine.")
  }
}

# Precisamos ao menos de quadprog para os problemas quadráticos
af_attach_packages(c("quadprog"))

# -------------------------------------------------------------------
# Config default
# -------------------------------------------------------------------

af_port_default_config <- list(
  mode         = "min_var",  # "equal", "inv_vol", "min_var", "mean_var", "max_sharpe"
  long_only    = TRUE,
  w_max        = 0.20,       # máx 20% por ativo
  leverage_max = 1.0,        # sum(w) = 1; sem alavancagem
  risk_aversion = 5,         # para mean_var / max_sharpe (λ)
  rf_daily      = 0          # taxa livre de risco diária (pode vir do CDI / SELIC)
)

# -------------------------------------------------------------------
# Utilitários básicos de pesos
# -------------------------------------------------------------------

af_port_equal_weights <- function(symbols) {
  n <- length(symbols)
  if (n == 0L) stop("af_port_equal_weights: empty symbol set.")
  rep(1 / n, n)
}

af_port_inv_vol <- function(Sigma) {
  if (!is.matrix(Sigma)) stop("af_port_inv_vol: Sigma must be a matrix.")
  sig <- sqrt(diag(Sigma))
  inv <- 1 / sig
  inv[!is.finite(inv)] <- 0
  if (sum(inv) <= 0) {
    # Fallback: equal weights
    n <- length(sig)
    return(rep(1 / n, n))
  }
  inv / sum(inv)
}

# -------------------------------------------------------------------
# Resolver QP com quadprog (min_var / mean_var / max_sharpe)
# -------------------------------------------------------------------

af_quadprog_solve <- function(Sigma,
                              mu           = NULL,
                              mode         = c("min_var", "mean_var", "max_sharpe"),
                              long_only    = TRUE,
                              w_max        = 1,
                              leverage_max = 1,
                              risk_aversion = 5,
                              rf_daily     = 0) {
  mode <- match.arg(mode)

  if (!is.matrix(Sigma)) stop("af_quadprog_solve: Sigma must be a matrix.")
  N <- nrow(Sigma)
  if (N != ncol(Sigma)) stop("af_quadprog_solve: Sigma must be square.")

  # quadprog: solve.QP(Dmat, dvec, Amat, bvec, meq)
  # Objetivo canônico: min 1/2 w' D w - d' w
  Dmat <- 2 * Sigma

  if (mode == "min_var") {
    dvec <- rep(0, N)
  } else {
    if (is.null(mu) || length(mu) != N) {
      stop("af_quadprog_solve: mu must be length N for mean_var / max_sharpe.")
    }
    mu <- as.numeric(mu)

    if (mode == "mean_var") {
      # max (w'μ - λ w'Σw) ≈ min (λ w'Σw - w'μ)
      # → Dmat = 2 λ Σ, dvec = μ
      Dmat <- 2 * risk_aversion * Sigma
      dvec <- mu
    } else if (mode == "max_sharpe") {
      # Heurística: tratar Sharpe ~ (w'(μ - rf)) com penalização de risco
      # via λ -> similar a mean_var com μtilde = μ - rf
      mu_tilde <- mu - rf_daily
      Dmat <- 2 * risk_aversion * Sigma
      dvec <- mu_tilde
    }
  }

  # Restrições lineares: A^T w >= b (quadprog usa essa convenção)
  # 1) soma w_i = 1 -> representamos como igualdade via meq
  Aeq <- matrix(1, nrow = N, ncol = 1)
  beq <- 1

  # 2) w_i >= 0 se long_only
  A_ineq <- NULL
  b_ineq <- NULL
  if (long_only) {
    A_ineq <- diag(N)
    b_ineq <- rep(0, N)
  }

  # 3) w_i <= w_max -> -w_i >= -w_max
  if (!is.null(w_max) && w_max < 1) {
    A_ineq2 <- -diag(N)
    b_ineq2 <- rep(-w_max, N)
    if (is.null(A_ineq)) {
      A_ineq <- A_ineq2
      b_ineq <- b_ineq2
    } else {
      A_ineq <- rbind(A_ineq, A_ineq2)
      b_ineq <- c(b_ineq, b_ineq2)
    }
  }

  # 4) leverage_max: com long_only e sum(w) = 1, leverage_max = 1 já está embutido.
  # Se futuramente tiver shorts, aqui precisa de restrição extra na soma(|w_i|).

  # Montagem Amat / bvec
  if (!is.null(A_ineq)) {
    # Igualdade vira as primeiras colunas de Amat, com meq = 1
    Amat <- cbind(Aeq, -Aeq, t(A_ineq))
    bvec <- c(beq, -beq, b_ineq)
    meq  <- 1
  } else {
    Amat <- cbind(Aeq, -Aeq)
    bvec <- c(beq, -beq)
    meq  <- 1
  }

  res <- quadprog::solve.QP(
    Dmat = Dmat,
    dvec = dvec,
    Amat = Amat,
    bvec = bvec,
    meq  = meq
  )

  w <- as.numeric(res$solution)
  names(w) <- rownames(Sigma)
  w
}

# -------------------------------------------------------------------
# Função principal: mu, Sigma -> pesos
# -------------------------------------------------------------------

af_build_portfolio <- function(mu,
                               Sigma,
                               config = af_port_default_config) {

  if (is.null(Sigma) || !is.matrix(Sigma)) {
    stop("af_build_portfolio: Sigma must be a covariance matrix.")
  }

  # Se mu for NULL e o modo precisar de mu, erro informativo
  mode <- match.arg(config$mode,
                    c("equal", "inv_vol", "min_var", "mean_var", "max_sharpe"))

  symbols <- colnames(Sigma)
  if (is.null(symbols)) {
    symbols <- paste0("Asset", seq_len(nrow(Sigma)))
    colnames(Sigma) <- rownames(Sigma) <- symbols
  }

  # Para equal / inv_vol, mu pode ser NULL
  if (!is.null(mu)) {
    if (length(mu) != nrow(Sigma)) {
      stop("af_build_portfolio: length(mu) must equal nrow(Sigma).")
    }
    names(mu) <- symbols
  }

  if (mode == "equal") {
    w <- af_port_equal_weights(symbols)
  } else if (mode == "inv_vol") {
    w <- af_port_inv_vol(Sigma)
  } else {
    w <- af_quadprog_solve(
      Sigma         = Sigma,
      mu            = mu,
      mode          = mode,
      long_only     = config$long_only,
      w_max         = config$w_max,
      leverage_max  = config$leverage_max,
      risk_aversion = config$risk_aversion,
      rf_daily      = config$rf_daily
    )
  }

  # Se long_only, garante w_i >= 0 numérica
  if (config$long_only) {
    w[w < 0] <- 0
  }

  # Normaliza para somar 1
  if (sum(w) > 0) {
    w <- w / sum(w)
  } else {
    # fallback defensivo: equal weights
    w <- af_port_equal_weights(symbols)
  }

  names(w) <- symbols

  list(
    weights = w,
    Sigma   = Sigma,
    mu      = mu,
    config  = config
  )
}



###############################################################################
### FILE: autofinance_risk_models.R
###############################################################################
############################################################
# autofinance_risk_models.R
#
# Risk model layer: Σ (covariance) and μ (expected returns)
# Methods:
#   Covariance:
#     - "sample"      : sample covariance matrix
#     - "shrinkage"   : simple shrink-to-diagonal
#     - "garch_dcc"   : GARCH(1,1) + DCC covariance at t_end
#
#   Expected returns:
#     - "hist_mean"   : historical average daily return
#     - "momentum"    : last-H-days average return
#     - "var"         : VAR-based 1-step-ahead forecast
#
# All returns here are DAILY. No annualization inside this module.
############################################################

# --- Config / packages -----------------------------------------------------

if (!exists("af_attach_packages")) {
  if (file.exists("autofinance_config.R")) {
    source("autofinance_config.R")
  } else {
    stop("autofinance_config.R not found; please source it before using risk models.")
  }
}

af_attach_packages(c("data.table"))

# --- Utility: prepare wide return matrix -----------------------------------

# panel: data.table with at least columns: symbol, refdate, ret_simple,
#        optionally excess_ret_simple.
#
# symbols: optional character vector of tickers to include.
# end_date: optional Date or "YYYY-MM-DD"; if NULL use max(panel$refdate).
# window_years: how many years back from end_date to include.
# use_excess: if TRUE and column 'excess_ret_simple' exists, use it; otherwise 'ret_simple'.
# min_obs_ratio: minimum fraction of non-NA observations required per asset.
#
# Returns:
#   list(
#     returns    : matrix (T x N) of daily returns,
#     dates      : Date vector (length T),
#     symbols    : character vector of N symbols (columns of matrix),
#     start_date : Date (min date used),
#     end_date   : Date (max date used)
#   )

af_risk_prepare_returns <- function(panel,
                                   symbols       = NULL,
                                   end_date      = NULL,
                                   window_years  = 3,
                                   use_excess    = TRUE,
                                   min_obs_ratio = 0.7) {

  panel <- data.table::as.data.table(panel)

  if (!all(c("symbol", "refdate") %in% names(panel))) {
    stop("panel must contain at least 'symbol' and 'refdate' columns.")
  }

  # Choose returns column
  col_ret <- NULL
  if (use_excess && "excess_ret_simple" %in% names(panel)) {
    col_ret <- "excess_ret_simple"
  } else if ("ret_simple" %in% names(panel)) {
    col_ret <- "ret_simple"
  } else {
    stop("panel must contain 'ret_simple' or 'excess_ret_simple'.")
  }

  # Filter symbols
  if (!is.null(symbols)) {
    panel <- panel[symbol %in% symbols]
  }

  if (nrow(panel) == 0) {
    stop("No rows in panel after symbol filter.")
  }

  # Normalize refdate to Date
  panel[, refdate := as.Date(refdate)]

  # Determine end_date
  if (is.null(end_date)) {
    end_date <- max(panel$refdate, na.rm = TRUE)
  } else {
    end_date <- as.Date(end_date)
  }

  # Determine start_date from window_years
  # Simple approximation: 365 * years. If you want trading days adjust later.
  start_date <- end_date - as.integer(365 * window_years)

  panel <- panel[refdate >= start_date & refdate <= end_date]

  if (nrow(panel) == 0) {
    stop("No data in panel for the requested window.")
  }

  # Wide matrix: rows = dates, cols = symbols, values = returns
  wide <- data.table::dcast(
    panel,
    refdate ~ symbol,
    value.var      = col_ret,
    fun.aggregate  = mean,
    fill           = NA_real_
  )

  dates <- as.Date(wide$refdate)
  R_mat <- as.matrix(wide[, -1, with = FALSE])
  colnames(R_mat) <- names(wide)[-1]

  # Drop assets with too many NAs
  n_obs <- length(dates)
  non_na_counts <- colSums(!is.na(R_mat))
  keep <- non_na_counts >= min_obs_ratio * n_obs

  if (!any(keep)) {
    stop("All assets dropped by min_obs_ratio filter; relax constraints or extend window.")
  }

  drops <- setdiff(colnames(R_mat), colnames(R_mat)[keep])
  if (length(drops) > 0) {
    message("Dropping ", length(drops), " assets due to insufficient data: ",
            paste(drops, collapse = ", "))
  }

  R_mat <- R_mat[, keep, drop = FALSE]

  list(
    returns    = R_mat,
    dates      = dates,
    symbols    = colnames(R_mat),
    start_date = min(dates),
    end_date   = max(dates)
  )
}

# --- Covariance estimation --------------------------------------------------

# returns_mat: numeric matrix (T x N) of daily returns (already filtered).
# method:
#   - "sample"
#   - "shrinkage"
#   - "garch_dcc"
#
# shrink_lambda: weight for shrinkage to diagonal (0=no shrink, 1=only diagonal).
# shrink_target: "diagonal" or "identity"
#
# For garch_dcc:
#   - if garch_spec/dcc_spec are NULL, default specs are created inside.

af_cov_estimate <- function(returns_mat,
                            method         = c("sample", "shrinkage", "garch_dcc"),
                            shrink_lambda  = 0.1,
                            shrink_target  = c("diagonal", "identity"),
                            garch_spec     = NULL,
                            dcc_spec       = NULL) {

  method        <- match.arg(method)
  shrink_target <- match.arg(shrink_target)

  if (!is.matrix(returns_mat)) {
    stop("returns_mat must be a numeric matrix.")
  }

  if (ncol(returns_mat) < 2L) {
    stop("Need at least 2 assets to estimate covariance.")
  }

  # Remove rows that are all NA
  all_na_rows <- apply(returns_mat, 1L, function(x) all(is.na(x)))
  returns_mat <- returns_mat[!all_na_rows, , drop = FALSE]

  if (nrow(returns_mat) < 10L) {
    stop("Too few observations after removing NA rows for covariance estimation.")
  }

  if (method == "sample") {
    Sigma <- stats::cov(returns_mat, use = "pairwise.complete.obs")
    return(list(
      Sigma  = Sigma,
      method = "sample",
      meta   = list(
        n_obs    = nrow(returns_mat),
        n_assets = ncol(returns_mat)
      )
    ))
  }

  if (method == "shrinkage") {
    S <- stats::cov(returns_mat, use = "pairwise.complete.obs")
    p <- ncol(S)

    if (shrink_target == "diagonal") {
      F <- diag(diag(S), nrow = p, ncol = p)
    } else {
      avg_var <- mean(diag(S), na.rm = TRUE)
      F <- diag(avg_var, nrow = p, ncol = p)
    }

    lambda <- max(min(shrink_lambda, 1), 0)
    Sigma  <- (1 - lambda) * S + lambda * F

    return(list(
      Sigma  = Sigma,
      method = "shrinkage",
      meta   = list(
        n_obs         = nrow(returns_mat),
        n_assets      = p,
        shrink_lambda = lambda,
        shrink_target = shrink_target
      )
    ))
  }

  if (method == "garch_dcc") {
    af_attach_packages(c("rugarch", "rmgarch"))

    # DCC cannot handle NAs; drop rows with any NA
    complete_rows <- stats::complete.cases(returns_mat)
    R_cc <- returns_mat[complete_rows, , drop = FALSE]

    if (nrow(R_cc) < 30L) {
      stop("Too few complete observations for GARCH-DCC estimation.")
    }

    N <- ncol(R_cc)

    # Default univariate GARCH(1,1) spec if not provided
    if (is.null(garch_spec)) {
      garch_spec <- rugarch::ugarchspec(
        variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
        mean.model     = list(armaOrder = c(0, 0), include.mean = TRUE),
        distribution.model = "norm"
      )
    }

    # Multivariate spec
    uspec  <- rmgarch::multispec(replicate(N, garch_spec))
    if (is.null(dcc_spec)) {
      dcc_spec <- rmgarch::dccspec(
        uspec     = uspec,
        dccOrder  = c(1, 1),
        distribution = "mvnorm"
      )
    }

    dcc_fit <- rmgarch::dccfit(
      spec = dcc_spec,
      data = R_cc
    )

    rcov_arr <- rmgarch::rcov(dcc_fit)  # array (N x N x T)
    t_last   <- dim(rcov_arr)[3L]
    Sigma    <- rcov_arr[, , t_last, drop = TRUE]

    dimnames(Sigma) <- list(colnames(R_cc), colnames(R_cc))

    return(list(
      Sigma  = Sigma,
      method = "garch_dcc",
      meta   = list(
        n_obs          = nrow(R_cc),
        n_assets       = N,
        convergence    = dcc_fit@mfit$convergence,
        last_index_row = t_last
      )
    ))
  }

  stop("Unknown covariance method: ", method)
}

# --- Expected returns estimation -------------------------------------------

# returns_mat: numeric matrix (T x N) of daily returns.
# method:
#   - "hist_mean" : μ_j = mean(r_j)
#   - "momentum"  : μ_j = mean of last 'momentum_window' observations
#   - "var"       : μ_j = 1-step-ahead forecast from VAR(p)
#
# macro_xts: placeholder for future factor-based VAR; currently optional/unused.
#
# Returns:
#   list(mu = numeric vector length N, method=..., meta=list(...))

af_mu_estimate <- function(returns_mat,
                           method          = c("hist_mean", "momentum", "var"),
                           momentum_window = 63L,
                           var_lag         = 1L,
                           macro_xts       = NULL) {

  method <- match.arg(method)

  if (!is.matrix(returns_mat)) {
    stop("returns_mat must be a numeric matrix.")
  }

  N <- ncol(returns_mat)
  if (N < 1L) {
    stop("No assets in returns_mat.")
  }

  # Remove rows that are all NA
  all_na_rows <- apply(returns_mat, 1L, function(x) all(is.na(x)))
  R <- returns_mat[!all_na_rows, , drop = FALSE]

  if (nrow(R) < 10L) {
    stop("Too few observations after removing all-NA rows for μ estimation.")
  }

  # Helper: per-column last non-NA index
  last_non_na_idx <- function(x) {
    idx <- which(!is.na(x))
    if (length(idx) == 0L) return(NA_integer_)
    tail(idx, 1L)
  }

  if (method == "hist_mean") {
    mu <- colMeans(R, na.rm = TRUE)
    return(list(
      mu     = as.numeric(mu),
      method = "hist_mean",
      meta   = list(
        n_obs    = nrow(R),
        n_assets = N
      )
    ))
  }

  if (method == "momentum") {
    w <- max(1L, as.integer(momentum_window))
    Tn <- nrow(R)

    mu <- numeric(N)
    names(mu) <- colnames(R)

    for (j in seq_len(N)) {
      x <- R[, j]
      idx_last <- last_non_na_idx(x)
      if (is.na(idx_last)) {
        mu[j] <- NA_real_
      } else {
        idx_start <- max(1L, idx_last - w + 1L)
        window_vals <- x[idx_start:idx_last]
        mu[j] <- mean(window_vals, na.rm = TRUE)
      }
    }

    return(list(
      mu     = mu,
      method = "momentum",
      meta   = list(
        n_obs            = nrow(R),
        n_assets         = N,
        momentum_window  = w
      )
    ))
  }

  if (method == "var") {
    af_attach_packages("vars")

    # VAR cannot handle NAs; drop rows with any NA
    R_cc <- R[stats::complete.cases(R), , drop = FALSE]

    if (nrow(R_cc) < (var_lag + 5L)) {
      stop("Too few complete observations for VAR with lag = ", var_lag)
    }

    # Fit VAR(p)
    var_fit <- vars::VAR(
      R_cc,
      p    = var_lag,
      type = "const"
    )

    # 1-step-ahead forecast
    fcst <- stats::predict(var_fit, n.ahead = 1L)

    # fcst$fcst is a list, one element per series
    mu <- numeric(N)
    names(mu) <- colnames(R_cc)

    for (j in seq_len(N)) {
      series_name <- colnames(R_cc)[j]
      comp <- fcst$fcst[[series_name]]
      mu[j] <- comp[1L, "fcst"]
    }

    return(list(
      mu     = mu,
      method = "var",
      meta   = list(
        n_obs      = nrow(R_cc),
        n_assets   = N,
        var_lag    = var_lag,
        var_call   = var_fit$call
      )
    ))
  }

  stop("Unknown μ method: ", method)
}

# --- High-level wrapper: af_risk_build -------------------------------------

# This is the main entry point the portfolio/backtest engine will call.
#
# panel: data.table with symbol, refdate, ret_simple / excess_ret_simple.
# symbols: optional subset.
# end_date, window_years, use_excess, min_obs_ratio: same semantics as prepare_returns.
#
# cov_method: "sample", "shrinkage", "garch_dcc"
# mu_method : "hist_mean", "momentum", "var"
#
# Extra args are forwarded to af_cov_estimate / af_mu_estimate via ...,
# so you can pass shrink_lambda, momentum_window, var_lag, etc.
#
# Returns:
#   list(
#     mu        : numeric vector (N),
#     Sigma     : matrix (N x N),
#     symbols   : character (N),
#     start_date: Date,
#     end_date  : Date,
#     cov_meta  : list(...),
#     mu_meta   : list(...)
#   )

af_risk_build <- function(panel,
                          symbols       = NULL,
                          end_date      = NULL,
                          window_years  = 3,
                          use_excess    = TRUE,
                          min_obs_ratio = 0.7,
                          cov_method    = c("sample", "shrinkage", "garch_dcc"),
                          mu_method     = c("hist_mean", "momentum", "var"),
                          ...) {

  cov_method <- match.arg(cov_method)
  mu_method  <- match.arg(mu_method)

  prep <- af_risk_prepare_returns(
    panel          = panel,
    symbols        = symbols,
    end_date       = end_date,
    window_years   = window_years,
    use_excess     = use_excess,
    min_obs_ratio  = min_obs_ratio
  )

  R_mat   <- prep$returns
  sym_ret <- prep$symbols

  # Covariance
  cov_res <- af_cov_estimate(
    returns_mat    = R_mat,
    method         = cov_method,
    ...
  )

  # Expected returns
  mu_res <- af_mu_estimate(
    returns_mat    = R_mat,
    method         = mu_method,
    ...
  )

  # Align dimensions
  if (length(mu_res$mu) != ncol(cov_res$Sigma)) {
    stop("Dimension mismatch between μ and Σ; check returns matrix handling.")
  }

  names(mu_res$mu) <- sym_ret
  dimnames(cov_res$Sigma) <- list(sym_ret, sym_ret)

  list(
    mu         = mu_res$mu,
    Sigma      = cov_res$Sigma,
    symbols    = sym_ret,
    start_date = prep$start_date,
    end_date   = prep$end_date,
    cov_meta   = cov_res$meta,
    mu_meta    = mu_res$meta,
    cov_method = cov_method,
    mu_method  = mu_method
  )
}

# --- Optional config + convenience wrapper ---------------------------------
# Isto NÃO muda nenhuma lógica já existente. Só oferece uma forma
# de passar um config-list em vez de um monte de argumentos soltos.

af_risk_config_default <- list(
  window_years    = 3,
  use_excess      = TRUE,
  min_obs_ratio   = 0.7,
  cov_method      = "sample",     # "sample", "shrinkage", "garch_dcc"
  mu_method       = "hist_mean",  # "hist_mean", "momentum", "var"
  shrink_lambda   = 0.1,
  shrink_target   = "diagonal",
  momentum_window = 63L,
  var_lag         = 1L
)

af_risk_estimate <- function(panel_returns,
                             symbols = NULL,
                             end_date = NULL,
                             config = af_risk_config_default,
                             ...) {
  af_risk_build(
    panel          = panel_returns,
    symbols        = symbols,
    end_date       = end_date,
    window_years   = config$window_years,
    use_excess     = config$use_excess,
    min_obs_ratio  = config$min_obs_ratio,
    cov_method     = config$cov_method,
    mu_method      = config$mu_method,
    shrink_lambda  = config$shrink_lambda,
    shrink_target  = config$shrink_target,
    momentum_window = config$momentum_window,
    var_lag         = config$var_lag,
    ...
  )
}




###############################################################################
### FILE: autofinance_screener.R
###############################################################################
############################################################
# autofinance_screener.R  (VERSÃO ATUALIZADA)
# Métricas cross-section + ranking por classe
############################################################

af_screener_config_default <- list(
  lookback_days   = 252L,
  horizons_days   = c(21L, 63L, 126L, 252L),
  min_liquidity   = 5e5,
  min_days_traded = 0.8,        # 80% dos dias com negócio
  ibov_series_id  = "IBOV",     # se você salvar IBOV em macro_series
  usd_series_id   = "USD_BR",   # idem
  # Peso alto em momentum médio/longo; curto com peso menor.
  # Penalização forte de drawdown/ulcer; moderada de vol, beta, liquidez ruim.
  score_weights   = list(
    # Momentum (quanto maior melhor)
    ret_21d        = +0.3,
    ret_63d        = +0.6,
    ret_126d       = +0.9,
    ret_252d       = +1.0,

    # Risco / estabilidade (quanto menor melhor)
    vol_21d        = -0.4,
    vol_252d       = -0.7,
    max_dd         = -0.7,
    ulcer_index    = -0.8,
    avg_time_underwater = -0.3,

    # Liquidez (Amihud alto = pior, então peso negativo)
    amihud         = -0.5,

    # Sistema (se disponível)
    beta_ibov      = -0.2,  # penaliza beta de mercado muito alto
    beta_usd       = +0.1   # leve prêmio para hedge em dólar (opcional)
    # Se quiser incluir skew/kurt/var/cvar: adicione aqui, mas pense na direção.
  )
)

############################################################
# Filtro de liquidez em cima de prices_raw
############################################################

af_compute_basic_liquidity_filter <- function(con,
                                              min_liquidity,
                                              min_days_traded,
                                              lookback_start,
                                              ref_date) {
  af_attach_packages("data.table")
  q <- sprintf("
    SELECT symbol, refdate, vol_fin, qty
    FROM prices_raw
    WHERE refdate >= '%s' AND refdate <= '%s'
  ", lookback_start, ref_date)
  dt <- data.table::as.data.table(DBI::dbGetQuery(con, q))
  if (nrow(dt) == 0L) return(data.table::data.table(symbol = character(0)))

  dt[, refdate := as.Date(refdate)]
  data.table::setorder(dt, symbol, refdate)

  liq <- dt[, .(
    median_vol_fin    = stats::median(vol_fin, na.rm = TRUE),
    days_traded_ratio = mean(qty > 0, na.rm = TRUE)
  ), by = symbol]

  liq[is.na(median_vol_fin),    median_vol_fin := 0]
  liq[is.na(days_traded_ratio), days_traded_ratio := 0]

  liq_filtered <- liq[
    median_vol_fin    >= min_liquidity &
      days_traded_ratio >= min_days_traded
  ]

  liq_filtered
}

############################################################
# Métricas por ativo (multi-horizonte + risco completo)
############################################################

af_compute_symbol_metrics <- function(dt_sym,
                                      horizons_days,
                                      factor_returns = NULL) {
  # dt_sym: data.table(date, close_adj, ret_simple, excess_ret_simple, vol_fin, qty)
  # horizons_days: vetor de janelas (em nº de observações)
  # factor_returns: list com vetores alinhados: ibov, usd, etc. (opcional)

  n <- nrow(dt_sym)
  if (n < 20L) return(NULL)  # pouco dado

  last_idx <- n
  metrics <- list(symbol = dt_sym$symbol[1])

  # -------------------------
  # 1) Retornos / Momentum
  # -------------------------
  for (h in horizons_days) {
    if (h <= n) {
      idx <- (last_idx - h + 1):last_idx
      ret_window <- prod(1 + dt_sym$excess_ret_simple[idx], na.rm = TRUE) - 1
      metrics[[paste0("ret_", h, "d")]] <- ret_window
      vol_window <- stats::sd(dt_sym$excess_ret_simple[idx], na.rm = TRUE) * sqrt(252)
      metrics[[paste0("vol_", h, "d")]] <- vol_window
    }
  }

  # -------------------------
  # 2) Drawdown, Ulcer, underwater
  # -------------------------
  prices <- dt_sym$close_adj
  cummax_p <- cummax(prices)
  dd <- prices / cummax_p - 1
  max_dd <- min(dd, na.rm = TRUE)

  ui <- sqrt(mean((dd * 100)^2, na.rm = TRUE))  # Ulcer Index em pontos percentuais

  underwater <- dd < 0
  if (any(underwater, na.rm = TRUE)) {
    rleid <- data.table::rleid(underwater)
    seg <- data.table::data.table(
      rleid = rleid,
      underwater = underwater
    )[, .N, by = .(rleid, underwater)]
    lengths <- seg[underwater == TRUE]$N
    avg_underwater <- mean(lengths, na.rm = TRUE)
  } else {
    avg_underwater <- 0
  }

  metrics$max_dd              <- max_dd
  metrics$ulcer_index         <- ui
  metrics$avg_time_underwater <- avg_underwater

  # -------------------------
  # 3) Liquidez Amihud
  # -------------------------
  valid <- dt_sym$vol_fin > 0 & !is.na(dt_sym$ret_simple)
  if (any(valid)) {
    amihud <- mean(abs(dt_sym$ret_simple[valid]) / dt_sym$vol_fin[valid], na.rm = TRUE)
  } else {
    amihud <- NA_real_
  }
  metrics$amihud <- amihud

  # -------------------------
  # 4) Skew, Kurt, VaR, CVaR
  # -------------------------
  x <- dt_sym$excess_ret_simple
  x <- x[is.finite(x)]
  if (length(x) > 20L) {
    m <- mean(x, na.rm = TRUE)
    s <- stats::sd(x, na.rm = TRUE)
    if (s > 0) {
      skew <- mean(((x - m) / s)^3, na.rm = TRUE)
      kurt <- mean(((x - m) / s)^4, na.rm = TRUE)
    } else {
      skew <- NA_real_; kurt <- NA_real_
    }
    q <- stats::quantile(x, 0.05, na.rm = TRUE)
    var_95  <- q
    cvar_95 <- mean(x[x <= q], na.rm = TRUE)
  } else {
    skew <- kurt <- NA_real_
    var_95 <- cvar_95 <- NA_real_
  }
  metrics$skew    <- skew
  metrics$kurt    <- kurt
  metrics$var_95  <- var_95
  metrics$cvar_95 <- cvar_95

  # -------------------------
  # 5) Betas / correlações vs fatores se fornecidos
  # -------------------------
  if (!is.null(factor_returns)) {
    fr <- factor_returns
    safe_beta <- function(a, b) {
      ok <- is.finite(a) & is.finite(b)
      if (sum(ok) < 20L) return(NA_real_)
      v <- stats::var(b[ok])
      if (v <= 0) return(NA_real_)
      stats::cov(a[ok], b[ok]) / v
    }
    safe_corr <- function(a, b) {
      ok <- is.finite(a) & is.finite(b)
      if (sum(ok) < 20L) return(NA_real_)
      stats::cor(a[ok], b[ok])
    }
    if (!is.null(fr$ibov)) {
      metrics$beta_ibov <- safe_beta(dt_sym$excess_ret_simple, fr$ibov)
      metrics$corr_ibov <- safe_corr(dt_sym$excess_ret_simple, fr$ibov)
    }
    if (!is.null(fr$usd)) {
      metrics$beta_usd <- safe_beta(dt_sym$excess_ret_simple, fr$usd)
      metrics$corr_usd <- safe_corr(dt_sym$excess_ret_simple, fr$usd)
    }
  }

  data.table::as.data.table(metrics)
}

############################################################
# Função principal: af_run_screener()
############################################################

af_run_screener <- function(ref_date = Sys.Date(),
                            config = af_screener_config_default,
                            con = af_db_connect()) {
  on.exit(af_db_disconnect(con), add = TRUE)
  af_attach_packages("data.table")

  ref_date <- as.Date(ref_date)
  lookback_days <- config$lookback_days
  # para segurança, pegamos janela 2x maior para métricas
  lookback_start <- ref_date - lookback_days * 2

  # 1) filtro de liquidez com prices_raw
  liq <- af_compute_basic_liquidity_filter(
    con             = con,
    min_liquidity   = config$min_liquidity,
    min_days_traded = config$min_days_traded,
    lookback_start  = lookback_start,
    ref_date        = ref_date
  )
  if (nrow(liq) == 0L) {
    stop("af_run_screener: no liquid symbols found.")
  }
  symbols_liq <- liq$symbol

  # 2) painel ajustado + retornos
  panel <- af_build_adjusted_panel(con, symbols_liq, lookback_start, ref_date)
  panel_ret <- af_compute_returns(panel)

  # 3) fatores macro (ex: IBOV, USD) – assumindo que você salvou como séries de retorno ou níveis
  macro_needed <- unique(c(config$ibov_series_id, config$usd_series_id))
  macro_needed <- macro_needed[!is.na(macro_needed)]
  macro <- if (length(macro_needed) > 0) {
    af_get_macro_series(con, macro_needed, lookback_start, ref_date)
  } else {
    data.table::data.table()
  }

  factor_returns <- list()
  if (nrow(macro) > 0L) {
    macro_wide <- data.table::dcast(macro, refdate ~ series_id, value.var = "value")
    macro_wide <- macro_wide[order(refdate)]
    # se forem níveis, aqui você converte para retornos; se já forem retornos, só alinha.
    # por enquanto assumo que macro_series$value já está em retorno diário.
    if (!is.null(config$ibov_series_id) && config$ibov_series_id %in% names(macro_wide)) {
      factor_returns$ibov <- macro_wide[[config$ibov_series_id]]
    }
    if (!is.null(config$usd_series_id) && config$usd_series_id %in% names(macro_wide)) {
      factor_returns$usd <- macro_wide[[config$usd_series_id]]
    }
  }

  # 4) limitar para última janela exata de lookback_days por símbolo
  data.table::setorder(panel_ret, symbol, date)
  metrics_list <- list()
  for (sym in unique(panel_ret$symbol)) {
    dt_sym <- panel_ret[symbol == sym]
    # pega últimos lookback_days observações (ou todas se < lookback_days)
    if (nrow(dt_sym) > lookback_days) {
      dt_sym <- dt_sym[(.N - lookback_days + 1):.N]
    }
    fr_sym <- NULL
    if (length(factor_returns) > 0L) {
      fr_sym <- lapply(factor_returns, function(x) {
        if (length(x) >= nrow(dt_sym)) tail(x, nrow(dt_sym)) else NA_real_
      })
    }
    m <- af_compute_symbol_metrics(
      dt_sym,
      horizons_days  = config$horizons_days,
      factor_returns = fr_sym
    )
    if (!is.null(m)) metrics_list[[sym]] <- m
  }

  metrics <- data.table::rbindlist(metrics_list, fill = TRUE)

  # 5) anexar asset_type da assets_meta (se existir)
  meta <- data.table::as.data.table(
    DBI::dbGetQuery(con, "SELECT symbol, asset_type FROM assets_meta")
  )
  metrics <- meta[metrics, on = .(symbol)]

  # 6) escore com z-score por métrica
  w <- config$score_weights
  metrics[, score := 0]

  for (nm in names(w)) {
    if (!nm %in% names(metrics)) next
    x <- metrics[[nm]]
    if (all(is.na(x))) next
    mu <- mean(x, na.rm = TRUE)
    s  <- stats::sd(x, na.rm = TRUE)
    if (is.na(s) || s == 0) next
    z <- (x - mu) / s
    metrics[, score := score + w[[nm]] * z]
  }

  # 7) ranking geral e por tipo
  metrics[, rank_overall := rank(-score, ties.method = "first")]
  metrics[, rank_type    := rank(-score, ties.method = "first"), by = asset_type]

  list(
    full   = metrics[order(rank_overall)],
    by_type = split(metrics[order(asset_type, rank_type)], metrics$asset_type)
  )
}



